{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "documents=SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BaseCallbackHandler', 'BasePromptTemplate', 'Callable', 'ChatPromptTemplate', 'ComposableGraph', 'Document', 'DocumentSummaryIndex', 'GPTDocumentSummaryIndex', 'GPTKeywordTableIndex', 'GPTListIndex', 'GPTRAKEKeywordTableIndex', 'GPTSimpleKeywordTableIndex', 'GPTTreeIndex', 'GPTVectorStoreIndex', 'IndexStructType', 'KeywordTableIndex', 'KnowledgeGraphIndex', 'ListIndex', 'MockEmbedding', 'NullHandler', 'Optional', 'Prompt', 'PromptHelper', 'PromptTemplate', 'PropertyGraphIndex', 'QueryBundle', 'RAKEKeywordTableIndex', 'Response', 'SQLContextBuilder', 'SQLDatabase', 'SQLDocumentContextBuilder', 'SelectorPromptTemplate', 'ServiceContext', 'Settings', 'SimpleDirectoryReader', 'SimpleKeywordTableIndex', 'StorageContext', 'SummaryIndex', 'TreeIndex', 'VectorStoreIndex', '__all__', '__annotations__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'async_utils', 'base', 'bridge', 'callbacks', 'chat_engine', 'constants', 'data_structs', 'download', 'download_loader', 'embeddings', 'evaluation', 'get_response_synthesizer', 'get_tokenizer', 'global_handler', 'global_service_context', 'global_tokenizer', 'graph_stores', 'image_retriever', 'indices', 'ingestion', 'instrumentation', 'llama_dataset', 'llms', 'load_graph_from_storage', 'load_index_from_storage', 'load_indices_from_storage', 'logging', 'memory', 'multi_modal_llms', 'node_parser', 'objects', 'output_parsers', 'postprocessor', 'prompts', 'query_engine', 'question_gen', 'readers', 'response', 'response_synthesizers', 'schema', 'selectors', 'service_context', 'service_context_elements', 'set_global_handler', 'set_global_service_context', 'set_global_tokenizer', 'settings', 'storage', 'tools', 'types', 'utilities', 'utils', 'vector_stores']\n"
     ]
    }
   ],
   "source": [
    "# checking for functionalities inside llama_index\n",
    "from llama_index import core\n",
    "print(dir(core))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='458ba755-dd89-48df-aba7-9f379c03661c', embedding=None, metadata={'page_label': '1', 'file_name': 'Book-Chapter-IGI-Global-May-2022.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/Book-Chapter-IGI-Global-May-2022.pdf', 'file_type': 'application/pdf', 'file_size': 821937, 'creation_date': '2024-06-23', 'last_modified_date': '2022-12-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" A Robust Classification Approach for Character \\nDetection Using P300 -Based Brain -Computer Interface  \\n \\nDeepthi Hitesh Mehta (deepthihmehta14499@gmail.com)  \\nNational Institute of Technology,  Raipur, India  \\n \\nMohit Sarin  (mohitsarin98@gmail.com)  \\nNational Institute of Technology, Raipur, India  \\n \\nPraveen Kumar Shukla (praveenlnct98@gmail.com)  \\nVIT Bhopal University, India  \\n \\nShrish Verma (shrishverma@nitrr.ac.in)  \\nNational Institute of Technology, Raip ur, India  \\n \\nRahul Kumar Chaurasiya (rkchaurasiya@nitrr.ac.in)  \\nNational Institute of Technology, Raipur, India  \\n \\nABSTRACT  \\n \\nResearchers have been contributing to the brain -computer interface (BCI), which acts as a \\ndirect connection between the human brain and the computer that uses the P300 speller \\nparadigm to decode the response of the brain by stimulating a subject, involving  no muscular \\nmovements. This research uses BCI Competition III Dataset II, which uses a 6x6 character \\nmatrix paradigm for data collection purposes for two healthy subjects. The ensembles of \\nsupport vector machine (SVM) method of classification has been pro posed to surpass the \\nproblem of false detection, which is preceded by empirical mode decomposition (EMD) as the \\npreprocessing technique and the use of a stacked autoencoder for feature extraction and \\ncovariate shift adaptation by normalized principal compo nents as the feature selection method \\nfor better accuracy of the detected character. The experiment yields a better result than many \\nexisting methods; it produces an average accuracy of 98.75%.  \\n \\nINTRODUCTION  \\n \\nBrain -Computer Interface (BCI) is direct commun ication between the human brain and the \\ncomputer machine. BCI involves the decoding of brain responses without any involvement of \\nmuscular motions. BCI is a proven excellent mode of communication for people with \\nneurological disorders who cannot communicat e their emotions and feelings through \\nhand writing, speaking, or typing ( R.K. Chaurasiya  et al., 2016 ). One of the primarily  used \\ntechniques among all the BCI techniques is  E lectroencephalographic(EEG)  because of its \\nnon-invasive recording technique and cost-benefit ratio.  The communication in EEG is based \\non an event -related potential (ERP)which is generated, recorde d, and analyzed, resulting in \\nthe event that visually stimulates a subject. An elicited component that is the response to ERP \\nis the P300 s ignal. P300 ERP is a natural endogenous response that varies with stimulation \\ntype, subject matter, and expectations, but it is independent of stimulation's physical \\ncharacteristics. P300 speller consists of the following technical aspects -recording of EEG  \\nsignals, preprocessing, feature ex traction, and classification ( Akcakaya M et al. , 2014 ). \\n \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a4cc9642-f524-4938-9f74-8bc20e04c5af', embedding=None, metadata={'page_label': '2', 'file_name': 'Book-Chapter-IGI-Global-May-2022.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/Book-Chapter-IGI-Global-May-2022.pdf', 'file_type': 'application/pdf', 'file_size': 821937, 'creation_date': '2024-06-23', 'last_modified_date': '2022-12-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n P300 potential is observed when rare , and expected events occur at the central locations of \\nEEG measurement. P300 is the user' s brain signature which typically happ ens around 300 ms \\nafter the unusual occurrence ( Wolpaw JR  et al., 2002 ). A 6x6 matrix of characters is shown in \\nthe P300 speller. The job of the user is to focus his attention on the characteristics of a \\npredefined sentence, one character at a time. But due to the proposed paradigm, the user faces \\nproblems with crowding, exhaustion, and thus resulting in false detection. So, to surpass this \\nproblem, the Ensembled Support Vector Machine (ESVM) method of classification has been \\nproposed in this research which is preceded by Empirical Mode Decomposition (EMD) as the \\npreprocessing technique and use of a stacked autoencoder and have applied Covariate Shift \\nAdaptation by normalized pr incipal components as an optimiz ation techniq ue for selection of \\nextracted features for better accuracy after classification.  \\n \\n \\nLITERATURE REVIEW  \\n \\nThis research uses BCI Competition III Dataset II , which consists of P300 signals , and for the \\nfeature extraction , the research makes use of an autoencoder. Any autoencoder takes input in \\nthe form of images, so there is a need to convert the P300 signals into an  image.  In their paper  \\n(Azad et al., 2019 ), has converted the 1D signals/ vibrational signals to 2 -D signals  using \\nEmpirical Mode Decomposition (EMD) and utilizing the energy esteems. The main reasons \\nfor the use of EMD for the conversion are because of its adaptive nature and since it allows \\nthe projection of a non -stationary signal on to a time -frequency plane using mono -component \\nsignals.  \\n \\nThere ar e many ways to extract features;  this research uses an autoencoder for feature \\nextraction as the autoencoder is capable of removing redundant information. I t doesn’t require \\nlabeled information of the data to create a  model for feature extraction ( Md Shopon et al., \\n2016 ). There are many autoencoders available for feature extracti on. In their  research  (S. \\nKundu  et al., 2019 ), have used sparse and stacked sparse autoencoder for feature extraction , \\nwhich yielded good results but t ook more time for training and had  less co nsistency. ( Minmin \\nChen, 2014 ) proposed a Marginalized Denoising autoencoder that learns the mapping from \\ninput to the output but doesn’t learn representa tion. This autoencoder can only modify the \\ndomain but cannot be used for representational learning and related problems. Stacked \\nAutoendocer has recently evolved to provide a version of the raw data with very \\ncomprehensive and promising features to train a classifier in a particular  context and find \\nmore consistency t han raw information training ( Venkata Krishna Jonnalagadda, 2018 ). \\n \\nTo increase the accuracy , this  research uses Principal Component Analysis (PCA) along with \\nCovariate Shift Adaption (CSA).PCA is used for extract ing the  principal components ( Jolliffe, \\nI. T et al. , 2016 ). (Spüler, M  et al., 2012 )  paper has made use of the mentioned technique and \\nhas described that the technique extracts principle components using PCA and CSA is used to \\nnormalize extracted components to  the effect of non -stationarity  by displacing the window \\nover the results. There are various covariate s hift adaption methods. A. Satti  in (A. Satti et al., \\n2010) proposed a covariate shift adaption method that uses a polynomial function to ada pt the \\ndata accordingly resulting  from the estimation of the covari ate shift of the subsequent trial, for \\nthat it uses a polynomial of order 3.  \\n \\n(Chaurasiya et al. , 2015 ) have proposed an efficient P300 speller. They have performed their \\nexperiments on the Devnagiri s cript. The design  and architecture of the speller are  much \\nsimilar to that of BCI Competition III Dataset II.  Classification of P300 signals was performed \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c0bda455-9787-434e-8626-1f2f4071ed4e', embedding=None, metadata={'page_label': '3', 'file_name': 'Book-Chapter-IGI-Global-May-2022.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/Book-Chapter-IGI-Global-May-2022.pdf', 'file_type': 'application/pdf', 'file_size': 821937, 'creation_date': '2024-06-23', 'last_modified_date': '2022-12-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n using Ensembles of SVM in this research. They were successful in minimizing problems \\nrelated to multi -trial. Taking it one step further , (G. B. Kshirsagar et al., 20 20) proposed a \\nSingle -trial Character Detection using the Ensembles of  a Deep Convolutional Network. They \\nhave achieved an accuracy of 96.2% on Devnagiri based P300 speller . \\n \\nDATASET DESCRIPTION  \\n \\nThe dataset used is the BCI Competition III Dataset II. It is a complete record of P300 evoked \\npotentials, recorded with BCI20001 by Donchin et al. in 2000 and originally Farwell and \\nDonchin in 1988 using a paradigm. The dataset contained data  collected from 2 subjects \\ncollected in 5 sessions. The user was shown a 6x6 matrix of English alphabets and digits (0 -9) \\nand was asked to focus on the single character out of the 36 characters. The frequencies were \\nincreased successively and at random at 5.7Hz, for all rows and columns on this matrix. Out \\nof the 12 intensified rows and columns , a particular row and column  contain the desired \\ncharacter. The reactions evoked by these two of the 12 stimuli which contain the desired \\ncharacter differ from the stimuli which did not contain the desired character and are similar to \\nthose describ ed in Farwell and Donchin, 1988;  (Donchin et al., 2000 ).  \\nThe data was collected in 5 sessions , each having many runs. The subject focuses on a series \\nof characters in each run. The user display was as follows for the run of each character epoch: \\nthe matrix was seen for 2.5  s, and each character had the same intensity during that time. Each \\nrow and column was intensified for 100ms , and then the matrix was left blank for 75ms. \\nThere were  a total of 12 intensifications ; each of them was repeated 15 times for each \\ncharacter epoch, so there were a total of 180 intensifications (12 x 15=180) for each character \\nepoch. An epoch of each character followed for a period of 2.5sec, and then the mat rix was \\nkept blank. During this period, the user was told of the completion of this character , and the \\nemphasis was on the next character in the word on the top of the screen. The dataset had 4 \\ndata, two for each subject, one of two was training data , and the other was the test data of that \\nsubject. The training data was used to predict the character sequence in the test data. In the \\nend, the signals were passed through a bandpass filter from 0.1 -60 Hz (Farwell LA et al., \\n1988).  \\n \\n \\n \\n \\n \\n \\n \\n                           Figure 1: The P300 Speller Paradigm for data collection  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d156b697-0eb0-472c-bc06-4bd99d869054', embedding=None, metadata={'page_label': '4', 'file_name': 'Book-Chapter-IGI-Global-May-2022.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/Book-Chapter-IGI-Global-May-2022.pdf', 'file_type': 'application/pdf', 'file_size': 821937, 'creation_date': '2024-06-23', 'last_modified_date': '2022-12-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\n \\nPROPOSED METHODOLOGY  \\n \\nThe research makes use of BCI Competition III Dataset II , which consists of data with signals \\nin binary matrix form which need  to be converted into 2 -D grayscale images ; thus, the research \\nmakes use of Empirical Mode Decomposition (EMD) as the preprocessing technique. The \\napplication of EMD results in Intrinsic Mode Functions (IMFs);  out of all the IMFs , those with \\nhigher frequencies are only considered. Then the energy of the signals used for converting into \\n2-D images is calculated. Then the energy values are normalized, and the energy spectrum is \\nconverted into a 2 -D image. The research uses a stacked autoencoder for feature extraction \\nfrom 2 -D images, and classification is done using the Support Vector Machine (SVM) method.  \\n \\nFILTERING & PREPROCESSING  \\n \\nTo  avoid noise and to restrict our observation to  a particular range of frequencies , we have \\nused the  8th order Ch ebyshev Bandpass filter with a  lower cut -off frequency of 0.1 Hz and a \\nhigher cut -off frequency of 10 Hz. We have chosen this range because , as the study (Zhihua \\nYang  et al., 200 6) suggests , the P300 signal lies predominantly in this range of frequencies.  \\n \\n \\n \\nFigure 2:  Proposed Method  \\n \\nEMD  \\n \\nThis research makes  use of autoencoders which take  inputs only in the form of images. The \\ndataset used for the study is the BCI Competition III Dataset II which consists of the data in \\nthe form of signals which is not suitable as an input for the autoencoder. So, there arises a \\nneed to preprocess the data to make that suitable as an input for the  autoencoder.  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8f49bc3c-012b-42b5-a774-f0bd24f50709', embedding=None, metadata={'page_label': '5', 'file_name': 'Book-Chapter-IGI-Global-May-2022.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/Book-Chapter-IGI-Global-May-2022.pdf', 'file_type': 'application/pdf', 'file_size': 821937, 'creation_date': '2024-06-23', 'last_modified_date': '2022-12-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nEMD is a time -space adaptive analysis approach suitable for non -stationary, non -linear, and \\nstochastic signal processing. EMD involves breaking down a signal without leaving the time \\ndomain and was proposed as an integral part of the Hilbert –Huang transform (HHT). Hilbert -\\nHuang transform (HHT) works in two stages. In the first stage, EMD breaks up non -linear and \\nnon-stationary data into a band -limited component summary called Intrinsic Mode Functions \\n(IMF). The second stage involves producing an instantaneous frequency spe ctrum by applying \\nthe Hilbert transform to the results obtained in the first stage.  \\nThe energy of the wave signal has then been used to describe the essence of the signal more \\neffectively than using the numerical values of the time field of the signal. The n the energy \\nvalues are used to convert into a 2 -D image.  \\n \\n1. Segmentation of the signal into subparts called Frames.  \\n2. Calculating the size of the frame  which is the multiplication of duration of the frame and \\nsignal’s frequency  rate. \\nSize of the frame= frame duration x Frequency rate of the signal.  \\nDefining the size of the matrix - \\nThe height of the matrix is equal to the size of the frame (M) and width equates to the \\nnumber of frames (N) produced after segmentation.  \\nSize of the matrix= M x N \\n3. Keeping the energy  values into the cells of the matrix and transferring the frame values \\nvertically to the matrix. The first column holds the energy values of the first frame, the \\nfirst frame  value  is transferred  to the first cell of the first row, the second  frame  value  \\nis transferred to the first cell of the second row and so on, and the last frame value of the \\nfirst frame is transferred to the first cell of the last row. This process continues for \\nfurther frames. Since the height of the matrix equates  to the size of the f rame, all the \\nframe values get fit in the matrix and since the width of the matrix is equal to the number \\nof frames produced after segmentation, all the energy values get fit in the matrix.  \\n4. The values in the matrix are normalized in the range 0 -255, thus r educing the noise.  \\n2-D representation preserves time -domain signal characteristics , and even 2 -D signal \\ntexture characteristics can be extracted to detect the signal [4].  \\n \\n \\nFigure 3:  Conversion into a 2 -D matrix with the energy values of the sample  \\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e5ac413f-8c15-4b77-86e1-c5491aa4fd0c', embedding=None, metadata={'page_label': '6', 'file_name': 'Book-Chapter-IGI-Global-May-2022.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/Book-Chapter-IGI-Global-May-2022.pdf', 'file_type': 'application/pdf', 'file_size': 821937, 'creation_date': '2024-06-23', 'last_modified_date': '2022-12-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\n \\n \\nFigure 4:  Producing an image from the 2 -D matrix  \\n \\n \\n \\nFigure 5:  Conversion of signal into a grey image.  \\n \\n \\n \\nFeature Extraction Autoencoder  \\n \\nStacked Autoencoder is prominently u sed for denoising images and compression of extracted \\nfeatures to a minimum so that only the most important features will remain. It  is a feed -forward \\nnetwork that  is used to reproduce the result in the output.  \\nThe hidden layer will maintain all the useful information, as the bottleneck part is mu ch smaller \\nthan the input layer;  henc e it is used for data compression. The weights between the hidden \\nlayer and the reconstructed output layer, which is weighted towards the decoder side, are tied to \\nthe weights used in encoder sides. “Tied” means  that the weights towards the decoder side ar e \\nthe transpose of weights used for the encoder side.  \\n \\nIn this research, the output obtained after converting  the P300 signals into a 2 -D grayscale \\nimage is given as input to the  stacked autoencoder. Initially , 30 percent of the features for the \\ntest and 70 percent for training were chosen to analyze classification problems  better . This was \\nachieved to ensure that the model knows the P300 stimulus and can develop the parameters. \\nAnd then, all the training sets were passed together for better learning level s and precision after \\nthe parameters and the model had  been found.  \\nMATLAB neural network was used to implement this project. Parameters such as the number \\nof iterations, number of layers, and neurons of the stacked autoencoders were set empirically. \\nInitia lly, the project began with two layers and 1000 iterations. Still, as the work continued , \\nthe layers were increased to improve the  accuracy , and the iterations were reduced to 300 after \\nexamination to face the over -fitting condition. The investigation  resulted in an optimum \\nnumber of layers as follows:  \\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='23ce7a94-c398-49a6-bbf2-1af4ee550200', embedding=None, metadata={'page_label': '7', 'file_name': 'Book-Chapter-IGI-Global-May-2022.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/Book-Chapter-IGI-Global-May-2022.pdf', 'file_type': 'application/pdf', 'file_size': 821937, 'creation_date': '2024-06-23', 'last_modified_date': '2022-12-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 1. The inputs with 420 features were passed to  the autoencoder’s first  layer.  \\n2. The inputs with 420 features were shortened to 210 by the first layer of autoencoder , \\nwhich was trained to 210.  \\n3. Next, there was a 100 neuron  autoencoder.  \\n4. The 100 features were passed on to the next autoencoder with 50 neurons.  \\n5. And then , the 50 features were passed on to the next autoencoder with 20 neurons.  \\n6. The 20 features were passed  to the last autoencoder with ten  neurons.  \\n \\nThis clearly shows that to reduce the features from 420 to 10 there were 5 layers which were a \\ncombination of 5 autoencoders. Besides, other parameters such as L2Weight Regularization \\nand Sparsity  Regularization were set in the MATLAB default configuration.  \\n \\nFurther, the ten features were passed on to the softmax matrix with 200 iterations. So, stacked \\nautoencoders with the structure: 420 -210-100-50-10 were  achieved at the end (Vařeka et al., \\n2017 ) \\n \\n \\n \\nFigure 6:  Architecture of Stacked Autoencoder  \\n \\n \\n \\nCSA+PCA  \\n \\nThe uncorrelated variables obtained by orthogonal transformation of correlated variables are \\ncalled principal components , and this method is called the PCA method  (Andrzej Maćkiewicz  \\net al., 1993) . The sorting of the principal components depends on the variance , and the first \\nmajor component reflects the highest variance of the original data. After extracting  the \\nfeatures , when the power spectrum has been estimated for each channel , the normalization \\nmethod is applied. The dataset contains a matrix D with dimension s n to p in the case of n \\ntesting of training, and the numbers of p= (channels. bins) and D (i, j) features are the value of \\ntests i and j . \\n \\nFirstly, a PCA is employed to minimize dimensionality and remove non -stationary \\ncomponents for the covariate shift adaptation, followed by  the selection  of m principal \\ncomponents with the highest variance, thus resulting in a transformation matrix W(pxm) and a \\nP=D.W matrix representing the principal components of m. Further, a rectangular window is \\ndefined with length w, which moves through the data and norm alizes the value of P (i, j) in the \\nprevious w trial  with- \\n \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9ca73f0c-605d-4b8e-b702-d996a0a2e4ad', embedding=None, metadata={'page_label': '8', 'file_name': 'Book-Chapter-IGI-Global-May-2022.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/Book-Chapter-IGI-Global-May-2022.pdf', 'file_type': 'application/pdf', 'file_size': 821937, 'creation_date': '2024-06-23', 'last_modified_date': '2022-12-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='                                                                                              \\n                                     is used for all 𝑃(𝑖𝑗)̂ with  \\n \\n \\nESVM  \\n \\nIn this research work , the features extracted are applied to the ensembles of the SVM classifier \\n(Marc Claesen et al., 2014 ) as the input , and the final output is the sum of the results  from \\neach classifier which are normalized using min -max normalization. Different types of fe atures \\nselected for evaluation are then concatenated to represent the signal in a much better way in \\nterms of the ratio of information it holds.  \\n \\nMin-Max \\n \\nIf 𝑓𝑝 is the score for test data assigned by the pth classifier th en the min -max normalization  \\n(Patro et al., 2015 ) function is given as follows - \\n \\n                                            \\n    \\n𝑓𝑝 is the normalized score.  \\nThe scaling takes place between 0 and 1 because of the min -max normalization. The final \\noutput of the classifier is given as - \\n \\n \\n                                                                                               \\nJ-number of epochs.  \\nP-number of classifiers.  \\nThe intersection of the row and column gives the desired character.  \\n \\nRESULTS  \\n \\nFor every subject, they have provided test data that consists of 85 ch aracters and training data \\nthat consists of 100 characters. So, we have approximately 46% training data and 54% testing \\ndata individually for both the subjects. Hence, in total , they have provided four .mat files in  \\nsingle -precision format. As we have worked on a newer version of MATLAB, we have \\nconverted a single -precision format to a double -precision format.  \\n \\nIn this research , we used EMD as a preprocessing  technique;  and extracted features using \\nstacked autoencoders. We have optimized the obtained results using PCA - Corporate Shift \\nAnalysis. For the final classification, we have used Ensembles of SVM.  Further, we have used \\ntwo different architecture s of the model. The first is  preprocessing, feature extra ction using a \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='637a53be-79fe-4537-8f52-97a538dff4d2', embedding=None, metadata={'page_label': '9', 'file_name': 'Book-Chapter-IGI-Global-May-2022.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/Book-Chapter-IGI-Global-May-2022.pdf', 'file_type': 'application/pdf', 'file_size': 821937, 'creation_date': '2024-06-23', 'last_modified_date': '2022-12-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' stacked autoencoder, and classification using  ESVM. In the second method , we have applied \\nPCA+ CSA for effective feature selection and to improve the performance of our proposed \\nmodel.  \\n \\nFigure 7  summarizes the results using the first architec ture obtained after every epoch. For \\nsimplification , results are rounded off to the nearest one’s place. It is evident from the results \\nthat classification accuracy for subject A is significantly lower for the first epoch,  but it gained \\nfaster than subject  B. At the end of 1st, 5th, 10th, 15th, and 18th epoch we  have obtained the \\nsubject wise accuracies of A as 19.5%, 71.6%, 85.2%, 95.5%, and  98.0%, whereas for subject \\nB the results are 32.0%, 76.9%, 89.0%, 95.0%, and 97.5%  respectively. The average accurac y \\nobtained after 18 epochs is 97.75%.  \\nFigure 7 also compares result s we have obtained after including PCA +CSA for feature \\nselection before the classification using Ensembles of SVM with those obtained without CSA. \\nWe can see the results  obtained using thi s architecture is around 1 -1.5% better than the first \\narchitecture of the  proposed model. Using PCA+ CSA, we have obtai ned subject -wise \\nclassification accuracies for Subject A for epochs 1st, 5th, 10th, 15th, and 18th as 21.0%, \\n72.1%, 93.2%,98.4%, and 99.3 %. Similarly for Subject B it is 36.3%, 78.3%, 91.2%, 96.8%, \\nand 98.2%respectively. We can see that the average classification accuracy here is increased to \\n98.75%.  \\n \\nTable 1: Confusion after 18 epochs without CSA  \\n \\n \\n \\n \\n \\n \\n \\nTable 2:  Confusion after 18 epochs with CSA  \\n \\n \\n \\n \\n \\n \\nMainl y, the alphabets misclassified after 18 epochs lie in the same row or same columns. It \\nmeans either row or column is detected correctly for these misclassified symbols. This may be \\nbecause some of the signals which have been recorded using different subjects are co -related.  \\n \\nIn Table 1, H , which is misclassified as Z , is in the same column, Q , which is misclassified as \\nP, is in the same row, similarly for T , which is misclassified as Z, by subject B. On the \\ncontrary, Table 4 summarizes the classification result obtained using the architecture that \\nconsists of feature selection using PCA + CSA. It’s clear from the results that  the feature \\nselection technique helps to improve the model  accuracy.  \\n \\n \\n \\n \\n Subjects  Expected  Output  \\nA H Z \\n Q P \\nB T Z \\nSubjects  Expected  Output  \\nA Q P \\nB T Z ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='de95618d-4a92-4a9e-b488-8ebad0379356', embedding=None, metadata={'page_label': '10', 'file_name': 'Book-Chapter-IGI-Global-May-2022.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/Book-Chapter-IGI-Global-May-2022.pdf', 'file_type': 'application/pdf', 'file_size': 821937, 'creation_date': '2024-06-23', 'last_modified_date': '2022-12-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 7:  Graph showing results with CSA and without CSA  \\n \\n \\nTable 3:  Paired T -test \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nHypothesis testing is one of the oldest procedures for checking data  validity . The T -test is one \\nof those statistical methods. In the beginning, the t -test implies a null hypothesis meaning the \\ntwo data set means are identical. Based on the test's predefined method, specific values are \\ncalculated and compared with standard values, and the null hypothesis is finally accepted or \\nrejected depending on the calculated values. If the null hypothesis appears to be dismissed, the \\ncollection of evidence is solid and is therefore not by mistake.  \\nIn this data, we have applied, paired T -test, a nd checked the value p -value. As it is clear from \\nTable 3, that all of the results of the paired t -test give the p -value , which is significantly less \\nthan 0.05. H ence this set of data passes the t -test. This test also validates the performance of \\nour model , as it shows improvement in c haracter recognition compar ed to earlier reported \\ntechniques by other researchers.  \\n \\nDISCUSSIONS  \\n \\nFor further analysis of the results obtained, we have compared our results with the results of \\nour fellow researchers. Many great  researchers have worked in this field and specifically on \\nthis dataset, they have used various methods for pre -processing, feature extraction, channel \\nselection , and classification. Some have reported average classification accuracy, whereas few \\nreported subject -wise accuracy . A few experiments have targeted artifact removal in pre -\\nprocessing, whereas others targeted proper channel selection. In this section, we have \\ncompared feature extraction techniques, channel selection techniques, and classification \\nmethods.  \\n Method  P-value  \\nCNN -1 (H. Cecotti  et al., 2011)  8.735 10-6 \\nMCNN -1 (F. Reza  et al.,2012)  4.255 10-5 \\nTemporal -ESVM (S. Kundu  et al., \\n2018)  2.78 10-3 \\nPCA -EWSVM (V. Guigue , 2008)  5.669 10-6 \\nESVM (M. J.  Idaji  et al., 2017)  4.89 10-4 \\n\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a0dd4a61-31eb-48f0-8ca6-9a20b187b953', embedding=None, metadata={'page_label': '11', 'file_name': 'Book-Chapter-IGI-Global-May-2022.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/Book-Chapter-IGI-Global-May-2022.pdf', 'file_type': 'application/pdf', 'file_size': 821937, 'creation_date': '2024-06-23', 'last_modified_date': '2022-12-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' Table 4 provides a detailed comparison of different methods that different researchers used to \\nsolve the proposed classification problem for BCI Competition III dataset II. Authors [25] used \\nensembles of SVM to classify this data; they have extrac ted features using a proposed deep \\nneural network. He (H. Cecotti  et al., 2011)  has used CNN with 16 hidden layers, extracted \\ndeep features using CNN, and performed classification using the CNN network.  \\nHe (S. Kundu et al., 2018 ) used PCA as a feature extraction method and ensembles of SVM as \\na classification method and has obtained a classification accuracy of 99% & 97% for subjects \\nA & B, respectively. In the (Mina Jamshidi Idaji , 2017 )applied LDA for classification of these \\nsignals, and the results were comparatively less accurate than deep neural networks. Deep \\nneural networks are more effective in this classification problem as they extract more relevant \\nfeatures. Moreover, the results are much more optimized because of the use  of autoencoders for \\nextracting features, as this would also include temporal features, and hence much better feature \\nextraction. Autoencoders have already been used by  (Liu et al., 2017) & (S. Kundu et al., \\n2019, and they have reported an average classif ication accuracy of 98% & 98.5%. We have \\nextended his (S. Kundu et al., 201 9) thoughts of using autoencoder but with autoencoder, we \\nhave also applied PCA+CSA for effective feature selection to improve classification results.  \\n \\nTable 4: Comparison of classification accuracy  \\n \\nMethod    Subject A  Subject B  Mean  \\nProposed \\nMethod  99.3 98.2 98.75  \\nCNN -1(H. \\nCecotti  et al., \\n2011)  97.0 92.0 94.5 \\nMCNN -1(H. \\nCecotti  et al., \\n2011)  97.0 94.0 95.5 \\nTemporal -ESVM  \\n(F. Reza  et al., \\n2012)  99.0 97.0 98.0 \\nPCA -EMSVM \\n(V. Guigue  et al., \\n2008)  99.0 97.0 98.0 \\nESVM (Mina et \\nal.,2017 )  97.0 96.0 96.5 \\nHOSRDA&LDA  \\n(Mina et al.,2017 \\n) 96.0 97.0 96.5 \\nGsBLDA  (T. Yu \\net al., 2015)  99.0 95.0 97.0 \\nBN3  ( Liu et al., \\n2017 ) 98.0 95.0 96.5 \\nSSAE -ESVM (S. \\nKundu  et al., \\n2019 ) 99.0 98.0 98.5 \\n                             \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bd173aed-3371-4f1a-af82-f2a49a20d98f', embedding=None, metadata={'page_label': '12', 'file_name': 'Book-Chapter-IGI-Global-May-2022.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/Book-Chapter-IGI-Global-May-2022.pdf', 'file_type': 'application/pdf', 'file_size': 821937, 'creation_date': '2024-06-23', 'last_modified_date': '2022-12-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n        \\n \\n \\n \\n \\n \\n \\n \\n   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n CONCLUSION & FUTURE SCOPE  \\n \\nIn this set of experiments, we have improved the classification accuracy of BCI Competition III \\ndataset II. For this, we have implemented a stacked autoencoder for the extraction  of   features  \\nand  PCA  +CSA  for  feature  selection  and the classification was performed using the \\nensemble of SVM and the overall results are represented in normalized form using min -max \\nnormalization. Feature extraction using  a Stacked autoencoder , we have obtained an average \\naccuracy of 98.75 % when we have used this model  architecture , compared  to the accuracy of \\n97.75 % when PCA +CSA is not included in the architecture of the model. Ap proximately an \\nincrease of [1 -1.5] % is recorded when we have used the feature selection compared to when \\nnot. Paired t -test with other proposed methods was also performed , and the results  are \\nsatisfactory as all of them  give a p -value less than 0.05. A de tailed comparis on of results \\nobtained and that obtained by other research has  been done, in terms of classification method \\nand preprocessing techniques which they have used. Those alphabets are also identified , which \\nhave been misclassified after 18 epochs  and have been listed in Table  4. \\nIn the future, we can try improving the accuracy close to 100% by using different channel \\nselection algorithm s to reject those channels that have more noise. We will also try to ext end \\nthis research to more than two  subjects and compare the performance of our model in that case. \\nFurther, we plan to implement our proposed model in an online system. Also, we are working \\nto improve accuracy using lower sequences r trails.  \\n \\n \\nCONFLICT OF INTEREST  \\n \\nThe authors declare that  there is no conflict of interest regarding this work.  \\n \\nACKNOWLEDGMENT  \\n \\nThe authors extend their appreciation and gratitude to the Electronics and Communication \\nDepartment, National Institute of Technology, Raipur , for supporting this research work.  \\n \\n \\nREFERENCES  \\n \\n Rahul Kumar Chaurasiya, Narendra D. Londhe & Subhojit Ghosh . (2016) . Binary DE -Based \\nChannel Selection and Weighted Ensemble of SVM Classification for Novel Brain –\\nComputer Interface Using Devanagari Script -Based P300 Speller Paradigm,  International \\nJournal of Human –Computer Interaction,  32:11,  861-877 \\nAkcakaya M, Peters B, Moghadamfalahi M, Mooney AR, Orhan U, Oken B, Erdogmus D, \\nFried -Oken M.  (2014).  Noninvasive brain -computer interfaces for augmentative and \\nalternative communication. IEEE Rev Biomed Eng. 2014 ;7:31 -49. \\nWolpaw JR, Birbaumer N, McFarland DJ, Pfurtscheller G, Vaughan TM.  (2002)  Brain -\\ncomputer   interfaces for communication and control. Clin Neurophysiol.; Page 767-91.  \\nAzad, M,Kh aled, F,Pavel, Monirul.(2019).  A novel approac h to classify and convert 1D signal \\nto 2D grayscale image implementing support vector machine and empirical mode  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='69832fe1-d105-407a-a70c-01b0b1f36530', embedding=None, metadata={'page_label': '13', 'file_name': 'Book-Chapter-IGI-Global-May-2022.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/Book-Chapter-IGI-Global-May-2022.pdf', 'file_type': 'application/pdf', 'file_size': 821937, 'creation_date': '2024-06-23', 'last_modified_date': '2022-12-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" decomposition algorithm .,International Journal of Advanced Research  \\nMd Shopon and Nabeel M ohammed and Md. Anowarul Abedin. ( 2016 ). Bangla handw ritten \\ndigit recognition using autoencoder and deep convolutional neural network, International \\nWorkshop on Computational Intelligence  (IWCI)},64 -68 \\nKundu, Sourav, Ari, Samit. (2019). P300 based character recognition using sparse autoencoder \\nwith ensemble of SVMs Biocybernetics and Biomedical Engineering , Volume 39  \\nMinmin Chen, Kilian Weinberger, Fei Sha, Yoshua Bengio . (2014 ). Marginalized Denoising \\nAuto -encoders for Nonlinear Represent ations, Proceedings of the 31st International \\nConference on Machine Learning, PMLR  32(2):1476 -1484,.   \\nVenkata Krishna Jonnalagadda. (2018).  Sparse, Stacked and  Variational Autoencoder, \\nhttps://medium.com/  \\nJolliffe, I. T., & Cadima, J. (2016). Principal component analysis: a review and recent \\ndevelopments.  Philosophical transactions. Series A, Mathematical, physical, and \\nengineering sciences , 374(2065)   \\nSpüler, M., Rosenstiel, W. & Bogdan. (2012). M. Principal component based covariate shift \\nadaption to r educe non -stationarity in a MEG -based brain -computer interface.  EURASIP \\nJ. Adv. Signal Process.  \\nA. Satti, C. Guan, D. Coyle and G. Prasad. ( 2010 ). A Covariate Shift Minimisation Method to \\nAlleviate Non -stationarity Effects for an Ada ptive Brain -Computer Interface,  20th \\nInternational Conference on Pattern Recognition  \\nR. K. Chaurasi ya, N. D. Londhe and S. Ghosh. ( 2015 ). An efficient P300 speller system for \\nBrain -Computer In terface,  International Conference on Signal Processing,  Computing \\nand Control (ISPCC)  \\nG. B. Kshirsagar and N. D. Londhe. (2020). Weighted Ensemble of Deep Convolution Neural \\nNetworks for Single -Trial Character Detection in Devana gari-Script -Based P300 Speller  \\nIEEE Transactions on Cognitive and Developmental Systems,  vol. 12, no. 3, pp. 551 -560,  \\nFarwell LA, Donchin E. ( 1988 ). Talking off the top of your head: toward a mental prosthesis \\nutilizing event -related brain potentials. Electroencephalogr Clin Neurophysiol.   \\nThulasidas M, Guan C, Wu J.  (2006 ). Robust classification of EEG signal for brain -computer \\ninterface. IEEE Trans Neural Syst Rehabil Eng.   \\nWadsworth BCI Dataset (P300 Evoked Potentials),Data Acquired Using BCI2000's P3 Speller \\nParadigm . (2004).  (http://www.bci2000.org)   \\nZhihua Yang, Lihua Y ang, Dongxu Qi, and Ching Y. Suen. (2006 ). An EMD -based \\nrecognition method fo r Chinese fonts and styles. Pattern Recognition Letters, Volume 27  \\nVařeka Lukáš, Ma utner Pavel. (2017)  Stacked Autoencoders for the P300 Component \\nDetect ion, Frontiers in Neuroscience  \\nAndrzej  Maćkiewicz, Waldemar Ratajczak. ( 1993 ) Principal components analysis \\n(PCA), Computers & Geosciences,  Volume 19, Issue 3 ,Pages 303 -342 \\nMarc Claesen,Frank De Smet, Johan A.K. Suykens,Bart De Moor. (2014).  EnsembleSVM: A \\nLibrary for Ensemble Learning Using Support Vector Machines , Journal of Machine \\nLearning Research , Page: 141-145 \\nPatro, S Gopal  Sahu, Dr -Kishore Kumar. ( 2015 ),Normalization: A Preprocessing Stage , \\nInternational Advanced Research Journal in Science, Engineering and Tec hnology  \\nH. Cecotti and A. Graser, . (2011) Convolutional Neural Networks for P300 Detection with \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b886873c-ff81-4b5d-a50b-42969a1bd2d2', embedding=None, metadata={'page_label': '14', 'file_name': 'Book-Chapter-IGI-Global-May-2022.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/Book-Chapter-IGI-Global-May-2022.pdf', 'file_type': 'application/pdf', 'file_size': 821937, 'creation_date': '2024-06-23', 'last_modified_date': '2022-12-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' Applicatio n to Brain -Computer Interfaces, IEEE Transactions on Pattern Analysis and \\nMachine Intelligence, vol. 33, no. 3, pp. 433 -445 \\nFazel -Rezai  Reza, Allison Brendan, Guger Christoph, Sellers E ric, Kleih Sonja, Kübler \\nAndrea. ( 2012 ). P300 brain computer interface: current challenges and emerging trends, \\nFrontiers in Neuroengineering,  Volume -5   \\nS. Kund u and S. Ari. (2018). P300 Detection Using En semble of SVM for Brain -Computer \\nInterface Application, 9th International Conference on Computing, Communication and \\nNetworki ng Technologies (ICCCNT)  \\nA. Rakotomamonjy and V. Guigue. (2008) BCI Competition III: Dataset II - Ensembl e of \\nSVMs for BCI P300 Spel ler, IEEE Transactions on Biomedical Engineering , vol. 55, no. \\n3, pp. 1147 -1154  \\nMina Jamshidi Idaji , Mohammad B. Shamsol lahi, Sepideh Hajipour Sardouie. ( 2017 ). Higher \\norder spectral regression discriminant analysis (HOSRDA): A tensor feature reduction \\nmethod for ERP detection, Journal of Pattern Recognition, Elsevier   \\nT. Yu, Z. Y u, Z. Gu and Y. Li. (2015). Grouped Automatic Relevance Determination and Its \\nApplication in C hannel Selection for P300 BCIs, IEEE Transactions on Neural Systems \\nand Rehabilitation Engineering , vol. 23, no. 6, pp. 1068 -1077  \\nLiu, Mingfei, Wu, Wei, Gu, Zhenghui, Yu, Zhul iang, Qi, FeiFei, Li, Yuanqing. (2017). Deep \\nLearning Based on Batch Normalization for P300 Signal Detection , Neurocomputing , \\nVolume 275,  Pages 288 -297 \\nSourav Kundu and Samit Ari. (2019).  P300 based character recognition using sparse \\nautoencoder with ensemble of SVMs, Biocybernetics and Biomedical Engineering  \\n \\n  \\n \\n.  \\n \\n \\n \\n \\n \\n \\n   \\n \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c470f903-5f18-475e-a0ad-e2a55b994e58', embedding=None, metadata={'page_label': '1', 'file_name': 'IEEE-Conference-Paper-ICCIKE-Dec-2019.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/IEEE-Conference-Paper-ICCIKE-Dec-2019.pdf', 'file_type': 'application/pdf', 'file_size': 1536792, 'creation_date': '2024-06-23', 'last_modified_date': '2022-09-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Face and Human Detection in Low Light for\\nSurveillance Purposes\\nMohit Sarin\\nElectronics and Communication Engg.\\nNIT Raipur\\nChhattisgarh, India\\nmohitsarin98@gmail.comShreya Chandrakar\\nElectronics and Communication Engg.\\nNIT Raipur\\nChhattisgarh, India\\nschandrakar.266@gmail.comDr. Ramnarayan Patel\\nElectrical Engg.\\nNIT Raipur\\nChhattisgarh, India\\nrnpatel.ee@nitrr.ac.in\\nAbstract —Surveillance based on computer vision is the need of\\nthe current era. Most of the existing surveillance cameras which\\nare being used by the Security Services and Military Forces\\nworks on the sensor-based analysis of conditions, it activates\\na signal indicating the presence of some movable creature and\\ncannot accurately differentiate between a hare, a deer or a\\nhuman. The current surveillance methods show high accuracy\\nin presence of daylight but fail miserably in low light conditions\\nbecause these sensors equipped with normal cameras are not able\\nto capture images with the same accuracy in such conditions.\\nHence to overcome this problem, we propose a network that is a\\ncombination of Human and Face Detection systems to distinguish\\nbetween different entities and enhance the accuracy of the results.\\nThis model works well in extreme conditions such as low-light,\\nblurred images, fog, haze or when most of the body or face is\\ncovered.\\nIndex Terms —human detection, convolutional neural network,\\nsurveillance methods, face detection\\nI. I NTRODUCTION\\nWhat differentiates a human from a machine is its capability\\nto differentiate between things, objects, faces, etc. But, in the\\npast few years machines have developed the capability to do\\nthe same. There are human and face detection systems that are\\nnot only able to detect but also differentiate between different\\nfaces and postures [1]. Earlier biometrics was the only method\\nfor a machine to clearly distinguish between two different\\nentities [2]. This included a ﬁngerprint sensing system for all\\nthe major control access systems, but with the evidence of\\ntechnology we have come a long way and iris-based access\\ncontrol has taken up the charge [3]. It surely doesn ´t end here\\nwith just biometrics as the only means of distinction. Now,\\nwe have Artiﬁcially Intelligent Face Recognition which can\\nbe embedded in systems as small as our Smartphones and\\nHome Security kits.\\nHuman Detection in the recent past has gained much recog-\\nnition and in a great way has complemented Face Detection\\ntechnology over a spectrum of uses. Many countries are using\\nthese technologies to detect and report accidents through an\\nautomated system that is set up at all the crowded crossings\\nand circles [4]. It is also being successfully implemented in\\nﬁelds like Autonomous Driving Cars, Rescue Services and\\nSurveillance Systems [5].\\nOut of all of its practical usage, what fascinated us the\\nmost was its applicability in the ﬁeld of Military Surveillance.The current technology that is being used is motion sensing\\n[6]. It is being used by most of the countries to keep a check\\non the borderline activities. But the precision remains a big\\nquestion mark due to the extreme conditions under which it\\nworks. So, incorporating technologies like low light human\\ndetection and blurred image sensing for face detection surely\\nwill add on to the accuracy with which we can detect any\\nactivity in such sensitive regions of work.\\nII. R ELATED WORK\\nA. Face Detection\\nInitial face detection work [7] was focused on robust\\nhandcrafted representations and included training of powerful\\nclassiﬁers for the application of machine learning. Around\\n2016, it utilized structural dependencies present in faces and\\nmodeled them using elastic deformation structures, which\\nseems to work well on the datasets present at that time, but its\\naccuracy decreases drastically when tested on UFDD Dataset\\n[8].\\nMany models have been proposed in the past years but the\\nViola-Jones face detector [9] is considered a breakthrough in\\nthe area of face detection. It uses Haar-like features, based on\\nthe AdaBoost Cascade scheme. Nowadays, researchers have\\nachieved high accuracy by integrating multiple hand-crafted\\nfeatures. Headhunter [10], ACF-multiscale [11] and LDCF+\\n[12] are some of the works which have achieved the ﬁnest\\nexecution among the traditional strategies. These solutions\\nare capable of real-time detection on CPU, but hand-crafted\\nfeatures lack the robustness to complicated face changes like\\nillumination, expression, pose and occlusion. Therefore, these\\nstrategies may not be versatile to low-quality images.\\nThe success of CNN-based methods in various computer\\nvision tasks such as object tracking, autonomous driving has\\ninspired several face detection approaches. Cascade CNN was\\nproposed to address the issue of high variances of face detec-\\ntion and high computational cost. In this algorithm, negative\\nsamples are removed at early stages and then the results are\\nreﬁned. Zhang et al. proposed a method named ICC-CNN [13],\\nwhich is similar to Cascade CNN but it rejects samples in\\ndifferent layers within a single CNN. The advantage of this\\nmethod is its high computation speed.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9a27f07d-e2f4-4f50-b118-21ddd94701ae', embedding=None, metadata={'page_label': '2', 'file_name': 'IEEE-Conference-Paper-ICCIKE-Dec-2019.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/IEEE-Conference-Paper-ICCIKE-Dec-2019.pdf', 'file_type': 'application/pdf', 'file_size': 1536792, 'creation_date': '2024-06-23', 'last_modified_date': '2022-09-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"The ﬁrst end-to-end CNN-based object detection method\\nwas Faster-RCNN [14]. It is based on a Region Proposal\\nNetwork (RPN) and a Region Classiﬁcation Network (RCN).\\nAnchor Boxes were ﬁrst proposed in Faster-RCNN, and this\\nmethod remains to be the base-line approach for most of the\\nanchor box based face detectors. The architecture of the RPN\\nnetwork is similar to that of VGG-16 [15].\\nBy the use of Dynamic Bayesian Network (DBN) [16],\\nGrgic et al. set up a few cameras to click facial images and\\nperform face recognition on it. Using a Principal Component\\nAnalysis (PCA) method [17], he mounted ﬁve cameras over\\nan entrance and collected face information at some different\\nlocations to perform face recognition.\\nAlgorithms based on SSD [18] has the advantage of the\\nmulti-scale feature maps, as it had prepared scale-variant\\ndetectors on diverse layers. One of the disadvantages of SSD is\\nthat it is not reasonable for identifying compact small objects,\\ndue to its default anchor design. S3FD [19], FaceBoxes [20],\\nScaleface [21] and HR-ER [22] have recently been proposed\\nto resolve the anchor mismatching issue and increase the\\nrecall level of small faces by either enhancing the matching\\nmethodology and anchor densities or assigning layers with\\nparticular scale ranges.\\nZhang et al. [23] suggested Single Shot Scale-Invariant\\nFace Detector (S3FD) to solve anchor-based techniques for\\nthe detection of small objects through their new methods of\\nanchor layout. To overcome the issue of a high false-positive\\nrate of small faces, authors have proposed a technique called\\nmax-out background technique. S3FD is yet another algorithm\\nfor object detection based on the Single Shot Detector (SSD)\\n[18] model for object detection, with VGG-16 [15] as the base\\nnetwork. To improve the detection accuracy, S3FD uses hard\\nnegative mining.\\nB. Human Detection\\nLayne et al. proposed a Symmetry-Driven Accumulation\\nof local features (SDALF) approach with Metric Learning\\nAttributes (MLA) [24].\\nIn the paper, Analysis-Based Gait Recognition for Hu-\\nman Identiﬁcation, by Wang, L.; Tan, T.; Ning, H.; Hu, W.\\nSilhouette [25], a study was conducted on the detection of\\nhuman identity gait based on PCA and silhouette analysis. In\\nStatistical Feature Fusion for Gait-Based Human Recognition\\nby Han J., Bhanu, [26], gait-based recognition was further\\nstudied and the problems arising due to insufﬁcient gait data\\nwere resolved. By applying Multiple Discriminant Analysis\\n(MDA) recognition performance was improved.\\nFor vision-related problems related to face or human iden-\\ntiﬁcation or detection, HOG methodology is widely applied.\\nSome of its applications can be seen in cases of Pedestrian\\nDetection, Age Estimation, Face Recognition and Gender\\nClassiﬁcation. Upon the accumulation of the strength and\\ndirection of the gradient information for all the pixels within\\nthe sub-block, this method constructs histogram features of an\\nimage sub-block. Generally, we apply the image enhancement\\nprocess while doing detection in low light conditions suchas night time. Several techniques have already been studied\\nfor image enhancement [27]. Traditional methodologies use\\nHistogram Speciﬁcation (HS) and Histogram Equalization\\n(HE). One of the problems with these methods was that of\\nan increase in the noise, which also results in a decrease of\\nthe important low-frequency components.To ﬁnd a solution\\nto these problems, numerous investigations have been done\\nand researches are being carried out on a variety of intensity\\nmapping-based image improvement techniques and histogram\\nprocessing. Numerous investigations have been done on dis-\\ntinctive strategies that perform De-Noising followed by image\\nenhancement [28]. Although, these methods have only shown\\npositive results to increase image visibility no concrete results\\nhave been put forth that overcomes the persisting issue of\\nhuman detection at night.\\nHuang's research has been successful in explaining human\\ndetection at night by the use of visible light images [29].\\nThe researchers used continuous visible light video frames\\nto perform human detection. As this method applies to con-\\ntinuous images of a person’s side, the only drawback that\\ncomes into light is that of an increased difﬁculty when the\\nperson is moving close to or far away from the camera. Since\\nthey need to process several continuous images the procedural\\ncomplexity and long processing time are some other signiﬁcant\\ndrawbacks. So, to overcome these problems we propose a\\ndeep CNN-based human detection model that processes upon\\na single image for both face and body data.\\nIII. P ROPOSED METHOD\\nOur proposed model is a 2-parallel set up to detect the\\nhuman body as well as a human face simultaneously. Our\\nmodel works well in extreme conditions such as low-light,\\nblurred images, fog, haze or when most of the body or face\\nis covered. So, it is well suited for surveillance in different\\nconditions.\\nWe have used a CNN based human detection network, with\\nvarious face detection techniques. The proposed model has\\nbeen experimented with the use of SSD [18], S3FD [19] and\\nHR-ER [22] method for face detection network.\\nA. The CNN Based Network\\nThe human detection network used in this paper is based\\non Convolutional Neural Network(CNN). The size of the\\ninput image generally depends on camera settings and hence\\nmay be different for different cameras. Hence, to ensure a\\nﬁxed size of the image goes as input to the proposed network,\\nnormalization is performed through Bilinear Interpolation to\\nﬁx the image size of 183 x 103 pixels (height x width). Most\\nof the famous networks such as AlexNet [30] and a few\\nothers from the previous work [31] used square input images.\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='927cd903-8275-4f6f-8d7f-86ca95570bb7', embedding=None, metadata={'page_label': '3', 'file_name': 'IEEE-Conference-Paper-ICCIKE-Dec-2019.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/IEEE-Conference-Paper-ICCIKE-Dec-2019.pdf', 'file_type': 'application/pdf', 'file_size': 1536792, 'creation_date': '2024-06-23', 'last_modified_date': '2022-09-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Start\\nInput Image\\nSize Normalization\\nHistogram Equalization\\nHR-ER Network CNN Network\\nClassifying whether human or not\\nStopFDN HDN\\nFigure 1. Overview of the model.\\nHDN - Human Detection Network\\nFDN - Face Detection Network\\nThe focus of this research is human area, for which\\nthe width is generally smaller than height. That’s why the\\nnormalization of the square-shape size is not done because\\nit stretches the picture to a great extent with regards to its\\nheight and width, that misshapes the human region and makes\\nit quite difﬁcult to detect features. Also if we use square\\nsize normalization without horizontal stretching too much\\nbackground area beside the targeted object (here human) will\\nbe included, which in turn results in incorrect detection of\\nfeatures. To prevent this, we have used uniform human or\\nbackground images as the input of CNN layers with 183 x\\n103 pixels (height x width). For incidence when the object\\nis either too close or far far away from the camera, size\\nnormalization proves to be the key to make up for the size\\nchange. To enhance the brightness of dark images, brightness\\nnormalization using the zero-center method was adopted.3-Channel\\nConv: 96@11*11*3; stride: 2*2\\nConv: 128@5*5*96; stride: 2*2\\nConv: 256@3*3*128; stride: 1*1\\nConv: 256@3*3*256; stride: 1*1\\nConv: 128@3*3*256; stride: 1*1\\nFully Connected Layer 1\\nFully Connected Layer 2\\nOutput Layer\\nFigure 2. Proposed CNN Model.\\nThe number of ﬁlters that are used in every convolutional\\nlayer used in our research is smaller as compared to that of\\nthe AlexNet structure. Moreover, the number of nodes in fully\\nconnected layers are also smaller than those in AlexNet. We\\nhave also compared the performances of the algorithm when\\nHistogram Equalization was performed and when it was not.\\nThe accuracy seems to be better when Histogram Equalization\\nwas done.\\nThe output of this CNN network gives a bounding box over\\nthe human. Hence, the image will be classiﬁed as a human and\\nbackground area. Figure 2 shows the structure of CNN used\\nin our experiment. All the details of convolutional layers and\\nfully connected layers are mentioned in Table I.\\nThe 1st convolutional ﬁlter size is relatively large as com-\\npared to the proposed size in previous studies [32,33] because\\ninput images are mostly present in low-quality, blurry and low-\\nlight conditions, so it has a greater amount of noise. Therefore,\\nthe ﬁlter size was increased to prevent invalid features from\\nbeing produced as a result of noise. The Rectiﬁed ReLu\\nlayer is used to avoid the Vanishing Gradient Problem that\\nmight occur when a sigmoid or hyperbolic function is used as\\nactivation in back-propagation for training.\\nAfter the Relu layer, we have used the cross channel', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e4877446-c590-4e83-99b5-e5880ec45537', embedding=None, metadata={'page_label': '4', 'file_name': 'IEEE-Conference-Paper-ICCIKE-Dec-2019.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/IEEE-Conference-Paper-ICCIKE-Dec-2019.pdf', 'file_type': 'application/pdf', 'file_size': 1536792, 'creation_date': '2024-06-23', 'last_modified_date': '2022-09-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='TABLE I\\nPROPOSED CNN A RCHITECTURE USED IN OUR MODEL .\\nLayer Name Number of\\nFiltersSize of Feature\\nMapSize of\\nKernelNumber of\\nStrideNumber of\\nPadding\\nImage input layer 183 x 103 x 3\\n1st convolutional layer 96 87 x 47 x 96 11 x 11 x 3 2 x 2 0 x 0\\nReLU Layer 87 x 47 x 96\\nCross Channel Normalization\\nLayer87 x 47 x 96\\nMax Pooling Layer 1 43 x 23 x 96 3 x 3 2 x 2 0 x 0\\n2nd Convolutional Layer 128 43 x 23 x 128 5 x 5 x 96 1 x 1 2 x 2\\nReLU Layer 43 x 23 x 128\\nCross Channel Normalization\\nLayer43 x 23 x 128\\nMax Pooling Layer 1 21 x 11 x 128 3 x 3 2 x 2 0 x 0\\n3rd Convolutional Layer 256 21 x 11 x 256 3 x 3 x 128 1 x 1 1 x 1\\nReLU Layer 21 x 11 x 256\\n4th Convolutional Layer 256 3 x 3 x 256 1 x 1 1 x 1\\nReLU Layer 21 x 11 x 256\\n5th Convolutional Layer 128 21 x 11 x 128 3 x 3 x 256 1 x 1 1 x 1\\nReLU Layer 21 x 11 x128\\nMax Pooling Layer 1 10 x 5 x 128 3 x 3 2 x 2 0 x 0\\n1st fully connected layer 4096\\nReLU Layer 4096\\n2nd fully connected layer 4096\\nReLU Layer 1024\\nDropout Layer 1024\\n3rd fully connected layer 1024\\nSoftmax Layer 2\\nClassiﬁcation Layer 2\\nnormalization layer to carry out channel-wise normalization.\\nThe input to the max-pooling layer was the feature map\\nobtained after cross channel normalization. The output of the\\nmax-pooling layer is a 96 feature map with a size of 43 x 23.\\nThe two initial layers were implied to extricate low-level\\npicture highlights that included edges of picture or blob texture\\nfeatures. Apart from these three additional layers were brought\\ninto consideration for high-level feature extraction.\\nAfter processing of the above mentioned ﬁve convolutional\\nlayers, 128 feature maps with a size of 21 x 11 pixels were\\nﬁnally obtained. The output of these ﬁve convolutional layers\\nwas given to three fully connected layers, which included\\n4096, 1024 and 2 neurons respectively. In the 3rd fully\\nconnected layer, we have used the softmax function.\\nAs mentioned in previous experimental results [30], the\\nover-ﬁtting problem is the most persistent issue that the CNN-\\nbased recognition systems suffer, which may result in low\\naccuracy output over the testing data, despite its high accuracy\\nwith training data. To overcome this issue, we used dropout\\nmethods [30] and data augmentation techniques to avoid the\\nproblem of over-ﬁtting. Before the 3rd fully connected layer,\\nwe have used a dropout layer.\\nTo disengage a few neurons connection between two con-\\nnected layers, the dropout method [30] was used with a\\ndropout probability of 50%.\\nIn the proposed model, both the networks (One for human\\ndetection and the other for face detection) were trained on a\\ndifferent dataset. The dataset used for training human detection\\nnetworks is CVC-14 [34], DNHD-DB1 [35] and Kaist datasets\\n[36]. And we have used the Dark Face open-source dataset[37], UFDD dataset [8] for the training of face detection\\nnetwork.\\nAs per ﬁgure 1, the input image will be common for both the\\nnetwork and the networks will run parallelly to give its output.\\nThe output from human detection network will be analyzed\\nﬁrst and if the human is not detected, as in some cases\\nonly face is visible (extreme condition when the human and\\nbackground color will be almost similar), then face detection\\nnetwork will give us the ﬁnal result indicating the presence or\\nabsence of the human.\\nFor the Face Detection algorithm, we tested our model\\nwith SSD [18], S3FD [19] and HR-ER [22]. We have used\\nthe architecture as proposed by their creators. Only we have\\nchanged the size of the input, which of (160 x 160) in S3FD,\\n[(40 x 40)-(140 x 140)] in HR-ER, but we have used (183 x\\n103).\\nThe performance of different algorithms on different\\ndatasets is compared and the overall performances of the\\nmodel with different networks can be seen in Table II.\\nAfter analyzing the performance of our model, different al-\\ngorithms for face detection were used and upon analysis, HR-\\nER was adopted, as it shows better results in all the datasets\\nwe have tried with. We have used a similar architecture as\\nproposed by Hu. It is based on RSNet-101. The input to this\\nalgorithm is the same as the input of the model as described\\nin Fig 2.\\nParticularly, when a person hides beside a bush or a tree so\\nthat to avoid coming completely under camera surveillance,\\nwe can detect his tiny face and conﬁrms that a human is\\npresent in that location. Moreover, HR-ER [22] will also work', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='37fab685-ac3a-4e25-adc0-6eb631abd247', embedding=None, metadata={'page_label': '5', 'file_name': 'IEEE-Conference-Paper-ICCIKE-Dec-2019.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/IEEE-Conference-Paper-ICCIKE-Dec-2019.pdf', 'file_type': 'application/pdf', 'file_size': 1536792, 'creation_date': '2024-06-23', 'last_modified_date': '2022-09-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"well in those cases where we want to keep the camera in a\\ndistant position so that it is not identiﬁed by a person. So, we\\nwill be having a very tiny image, and hence this algorithm\\noutperforms the other algorithms.\\nHence, it was concluded that CNN based human detection\\nworks best with HR-ER [22] face detection method.\\nB. Measurement Criteria\\nTo report the results of our experiment, the human or face\\narea is considered to be positive data whereas the background\\nis considered negative data. To simplify the criteria, we further\\ndivide the positive and negative data into True or False. When\\nthe background is correctly detected then it gives a True\\nNegative (TN) value high. Whereas, when a face or human\\nis detected it gives a high True Positive (TP) value. On the\\ncontrary, if the background is inaccurately reported, we get a\\nFalse Negative (FN) high and when an inaccurate human or\\nface is recognized as a high False Positive (FP) is obtained.\\nHere we have used true positive rate to deﬁne errors but,\\nrather False Negative Rate (FNR) and False Positive Rate\\n(FPR) can also be used for the same. The FNR can be\\ncalculated as 100 −TPR (%) and FPR as 100−TNR (%).\\nTable II list the TPR as confusion matrices for different\\ndatasets [8,34,35,36,37].\\nIV. R ESULTS AND DISCUSSIONS\\nFig. 3a Fig. 3b\\nFig. 3c Fig. 3d\\nFig. 3e Fig. 3f\\nFigure 3. Output Images\\nThe three models were tested on four different databases,\\nmostly on the low-light images for both human and face\\ndetection network.After analyzing the results it can be concluded that the\\nCNN+HR-ER model performs the best, with an average\\nTPR of 96.27%. Our Model is tested on different datasets\\n[8,34,35,36,37]. The results and comparisons are mentioned\\nin Table II. On human + face dataset, our model performs\\nappreciably in haze conditions and even for blur and low-\\nquality images.\\nThe results obtained by our proposed model in the extreme\\nconditions such as in a fog, dark and camouﬂaged background\\nis shown in Figure 3.\\nFig. 3a and Fig. 3b represents the performance of our\\nproposed network in the extreme weather conditions and\\nwhen there is not much difference between the background\\nand human. Detection in these conditions will be very useful\\nfor military and surveillance purposes.\\nWhile testing on face related datasets [8,36], the human\\ndetection algorithm doesn't perform well as expected, hence\\nonly the output of face detection algorithm was considered\\nand the presence of a face, indicates the presence of human\\ntoo. Although this is one of the rare cases, indicating that\\nthe surveillance camera is so much nearer to the human that\\nit can only capture its face as shown in Fig. 3c. When the\\ncaptured image contains complete human, for example in Fig.\\n3d or 3e, our algorithm works well.\\nFig. 3e indicates that our network works well in haze and fog\\nconditions.\\nV. F ALLACY OF THE MODEL PROPOSED\\nFig. 4a Fig. 4b\\nFigure 4. Fallacy of the Model\\nIn a very dark environment and no light condition, some-\\ntimes the background and human cannot be distinguished\\nappropriately as shown in image Fig. 4a. i.e. the image\\nbrightness is so low that it is impossible to detect human or\\nface information even by human eyes. Some of the images\\nwhere our algorithm doesn't perform well are shown in Fig.\\n4a and Fig. 4b.\\nAlso in some of the cases, when a surveillance camera is\\ntoo close to human or the cases where only some part of the\\nface is visible, some extreme cases such as Fig. 4b, which\\nrepresents a tattoo on a human hand which is held very close\\nto a surveillance camera, give misleading results.\\nAs we have proposed this model for surveillance purposes,\\nhence images such as that of Fig. 4b will be the rarest case.\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f799af23-dd27-4d21-8f0f-e7ce12f84bd0', embedding=None, metadata={'page_label': '6', 'file_name': 'IEEE-Conference-Paper-ICCIKE-Dec-2019.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/IEEE-Conference-Paper-ICCIKE-Dec-2019.pdf', 'file_type': 'application/pdf', 'file_size': 1536792, 'creation_date': '2024-06-23', 'last_modified_date': '2022-09-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"TABLE II\\nAVERAGE TESTING ACCURACY FOR HUMAN /FACE DETECTION\\nMETHOD DATABASE TPR\\nProposed CNN +HR-ER CVC + UFDD 96.83\\nKAIST + UFDD 95.93\\nDNHD-DB1 + Dark 96.02\\nDNHD-DB1 + UFDD 96.30\\nProposed CNN +SSD CVC + UFDD 92.80\\nKAIST + UFDD 91.13\\nDNHD-DB1 + Dark 91.22\\nDNHD-DB1 + UFDD 91.49\\nProposed CNN +S3FD CVC + UFDD 94.21\\nKAIST + UFDD 93.41\\nDNHD-DB1 + Dark 93.12\\nDNHD-DB1 + UFDD 93.77\\nVI. C ONCLUSIONS\\nThe above-proposed model performs human and face de-\\ntection one after the other. All the experiments done until\\nnow was done separately for human and face, but in practical\\nsituations, we need a model that performs both the task one\\nafter the other.\\nSo, the model integrates these two tasks and for military\\npurposes, it will be easy to detect humans in low light,\\nrain, haze, and blur. In cases where the human body is\\nhighly camouﬂaged with the background, the face recognition\\novercomes the issue of non-recognition of human despite it's\\npresence in the frame.\\nUpon testing the performance of the various combination\\nof networks while changing the blur, noise or contrast level.\\nWe have compared various models on publicly available\\ndatasets, for humans we have used DNHD-DB1, CVC-14 and\\nKAIST Dataset and for faces we have used Caltech, UMIST,\\nexclusively Dark Dataset and UFDD Dataset. On examining\\nvarious combinations, we state that the combination of the\\nproposed CNN network for the human + HR-ER method for\\nfaces gives the best result.\\nWe have compared different combinations in term of TPR\\n(True Positive Rate) and the results are listed in Table II.\\nREFERENCES\\n[1] H. Chung, C. Hou and S. Liang, ”Face detection and posture recognition\\nin a real time tracking system,” 2017 IEEE International Systems\\nEngineering Symposium (ISSE) , Vienna, 2017, pp. 1-6.\\n[2] Sharma, Annu and Chaturvedi, Praveena and Arya, Shwetank. (2015).\\nHuman Recognition Methods based on Biometric Technologies. Inter-\\nnational Journal of Computer Applications . 120. 1-7.\\n[3] A. Kumar and A. R. Asati, ”Iris based biometric identiﬁcation system,”\\n2014 International Conference on Audio, Language and Image Process-\\ning, Shanghai, 2014, pp. 260-265.\\n[4] Bhumkar, S. P., V . V . Deotare, and R. V . Babar. ”Accident avoidance\\nand detection on highways.” International Journal of Engineering Trends\\nand Technology 3.2 (2012): 247-252.\\n[5] Bojarski, Mariusz, et al. ”End to end learning for self-driving cars.”\\narXiv preprint arXiv:1604.07316 (2016).\\n[6] Gill, Tyler, et al. ”A system for change detection and human recognition\\nin voxel space using the Microsoft Kinect sensor.” 2011 IEEE Applied\\nImagery Pattern Recognition Workshop (AIPR) . IEEE, 2011.\\n[7] S. C. Brubaker, J. Wu, J. Sun, M. D. Mullin, and J. M. Rehg. On the\\ndesign of cascades of boosted ensembles for face detection. International\\nJournal of Computer Vision , 77(1- 3):6586, 2008.[8] Nada, Hajime, et al. ”Pushing the limits of unconstrained face detection:\\na challenge dataset and baseline results.” 2018 IEEE 9th International\\nConference on Biometrics Theory, Applications and Systems (BTAS) .\\nIEEE, 2018.\\n[9] Viola, Paul, and Michael J. Jones. ”Robust real-time face detection.”\\nInternational journal of computer vision 57.2 (2004): 137-154.\\n[10] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, Face detection\\nwithout bells and whistles, in European Conference on Computer Vision ,\\npp. 720735, Springer, 2014.\\n[11] B. Yang, J. Yan, Z. Lei, and S. Z. Li, Aggregate channel features for\\nmulti-view face detection, in IJCB, pp. 18, IEEE, 2014.\\n[12] E. Ohn-Bar and M. M. Trivedi, To boost or not to boost? on the limits of\\nboosted trees for object detection, in ICPR, pp. 33503355, IEEE, 2016.\\n[13] K. Zhang, Z. Zhang, H. Wang, Z. Li, Y . Qiao, and W. Liu, Detecting\\nfaces using inside cascaded contextual cnn, in ICCV , pp. 31713179,\\n2017.\\n[14] Girshick, Ross. ”Fast r-cnn.” Proceedings of the IEEE international\\nconference on computer vision . 2015.\\n[15] Han, Song, Huizi Mao, and William J. Dally. ”Deep compression:\\nCompressing deep neural networks with pruning, trained quantization\\nand huffman coding.” arXiv preprint arXiv:1510.00149 (2015).\\n[16] Murphy, Kevin Patrick, and Stuart Russell. ”Dynamic bayesian net-\\nworks: representation, inference and learning.” (2002).\\n[17] Wold, Svante, Kim Esbensen, and Paul Geladi. ”Principal component\\nanalysis.” Chemometrics and intelligent laboratory systems 2.1-3 (1987):\\n37-52.\\n[18] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and\\nA. C. Berg, Ssd: Single shot multibox detector, in ECCV , pp. 2137,\\nSpringer, 2016\\n[19] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li, S 3 fd: Single\\nshot scale-invariant face detector, arXiv preprint arXiv:1708.05237 ,\\n2017.\\n[20] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li, Face-\\nboxes: a cpu real-time face detector with high accuracy, arXiv preprint\\narXiv:1708.05234 , 2017.\\n[21] S. Yang, Y . Xiong, C. C. Loy, and X. Tang, Face detection through scale-\\nfriendly deep convolutional networks, arXiv preprint arXiv:1706.02863 ,\\n2017.\\n[22] P. Hu and D. Ramanan, Finding tiny faces, in CVPR, pp. 15221530,\\nIEEE, 2017.\\n[23] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li. S3fd: Single\\nshot scale-invariant face detector.\\n[24] Layne, R.; Hospedales, T.M.; Gong, S. Towards Person Identiﬁcation\\nand Re-Identiﬁcation with Attributes. pp. 402412.\\n[25] Wang, L.; Tan, T.; Ning, H.; Hu, W. Silhouette Analysis-Based Gait\\nRecognition for Human Identiﬁcation.IEEE Trans. Pattern Anal. Mach.\\nIntel. 2003, 25, 15051518\\n[26] Han, J.; Bhanu, B. Statistical Feature Fusion for Gait-Based Human\\nRecognition.pp. II-842II-847.\\n[27] Gonzalez, R.C.; Woods, R.E. Digital Image Processing, 3rd ed.; Prentice\\nHall: New Jersey, NJ, USA, 2010.\\n[28] Li, L.; Wang, R.; Wang, W.; Gao, W. A low-light image enhancement\\nmethod for both denoising and contrast enlarging. pp. 37303734.\\n[29] Huang, K.; Wang, L.; Tan, T.; Maybank, S. A real-time object detecting\\nand tracking system for outdoor night surveillance. Pattern Recognit.\\n2008, 41, 432444.\\n[30] Krizhevsky, A.; Sutskever, I.; Hinton, G.E. Imagenet classiﬁcation with\\ndeep convolutional neural networks.In Advances in Neural Information\\nProcessing Systems 25; Curran Associates, Inc.: New York, NY , USA,\\n2012; pp. 10971105.\\n[31] Lecun, Y .; Bottou, L.; Bengio, Y .; Haffner, P. Gradient-based learning\\napplied to document recognition. Proc. IEEE 1998, 86, 22782324.\\n[32] Simonyan, K.; Zisserman, A. Very deep convolutional networks for\\nlarge-scale image recognition. pp. 114.\\n[33] Szegedy, C.; Liu, W.; Jia, Y .; Sermanet, P.; Reed, S. Going deeper with\\nconvolutions. pp. 19.\\n[34] Gonzlez, A.; Fang, Z.; Socarras, Y .; Serrat, J.; Vzquez, D.; Xu, J.;\\nLpez, A.M. Pedestrian detection at day/night time with visible and FIR\\ncameras.\\n[35] http://dm.dgu.edu/link.html\\n[36] Hwang, S.; Park, J.; Kim, N.; Choi, Y .; Kweon, I.S. Multispectral\\npedestrian detection: Benchmark dataset and baseline. pp. 19.\\n[37] https://ﬂyywh.github.io/CVPRW2019LowLight/\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f4851d0c-a411-45be-805e-b90995ed625f', embedding=None, metadata={'page_label': '1', 'file_name': 'IEEE-Conference-Paper-SPIN-Feb-2020.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/IEEE-Conference-Paper-SPIN-Feb-2020.pdf', 'file_type': 'application/pdf', 'file_size': 1997126, 'creation_date': '2024-06-23', 'last_modified_date': '2022-09-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/Automated   Ocular   Artifacts   Identification   and  \\nRemoval   from   EEG   Data   Using   Hybrid   Machine  \\nLearning   Methods  \\nMohit   Sarin,   Akshat   Verma,   Deepthi   Hitesh   Mehta,   Praveen   Kumar   Shukla,   and   Shrish   Verma  \\nDepartment   of   Electr onics   and   Communication   Engineering   \\nNational   Institute   of   Technology   Raipur ,   India   \\nmohitsarin98@gmail.com ,    akshatv45@gmail.com ,    deepthihmehta14499@gmail.com ,    praveenlnct98@gmail.co m,    shrishverma@nitrr .ac.in  \\n \\n \\nAbstract —  Brain  Computer  Interface  (BCI)  is  an  emerging         \\nfield  of  resear ch,  which  helps neur omuscular  disabled  people         \\nto  communicate  and  contr ol  external  devices  with  the  brain.          \\nElectr oencephalogram  (EEG)  signals  have  been  used  by        \\ndoctors  to  diagnose  brain  activities.  EEG  signals  are  very          \\nsensitive  to  the  physical  movement  of  most  of  the  body  parts.            \\nElectr ooculogram  (EOG)  are  the  signals  which  are  directly         \\nrelated  to  eye  movements.  In  the  field  of  BCI,  artifacts  refer  to             \\nthose  signals  which  are  not  requir ed  but  will  be  present  in  data             \\nbecause  of  the  involuntary  actions  of  human  muscles.  In  the           \\nproposed  resear ch,  BCI  Competition  III  Dataset  IV  A  is  used,           \\nwhich  consists  of  two  classes  of  signals  right  hand  and  right            \\nfoot,  so  ocular  artifacts  were  a  prime  hindrance  for  this  type  of             \\ndata.  The  resear ch  involves  the  use  of  the  following  featur e           \\ndetection  methods:  ICA,  ICA-RLS,  ICA-LMS,  ICA-DWT ,       \\nEMD-ICA.  The  first  common  step  is  using  ICA  to  obtain           \\nindependent  components(ICs).  Using  ICA,  ICs  that  contain        \\nocular  artifacts  can  be  identified  and  then  further  exposed  to           \\nanother  round  of  ICA  or  adaptive  filtering  that  uses  LMS,           \\nRLS.  The  adaptive  filtering  technique  reduces  the  need  for          \\nparallel  EOG  recordings  by  removing  the  ocular  ICs  using          \\nreference  signals.  Hybrid`  methods  have  gained  much        \\nadvantage  over  the  traditional  ICA  process,  which  leads  to          \\nbetter  detection  and  removal  of  ocular  artifacts  with  these          \\ntechniques.  Further ,  the  resear ch  proposes  a  system  involving         \\nthe  use  of  SVM-PSO  for  the  classification  of  motor  imagery           \\nsignals  obtained  after  the  removal  of  artifacts.  ICA-RLS  and          \\nCSP  as  pre-processing  and  featur e  extraction  techniques        \\nprovided   the   best   classification   accuracy   of   92.94%.  \\nKeywords—Motor  imagery ,  ocular  artifacts,     \\nElectroencephalogram  (EEG),  Independent  Component     \\nAnalysis(ICA).  \\nI.   INTRODUCTION  \\nThe  Brain-Computer  Interface  (BCI)  is  a  new        \\ntechnology  adopted  by  people  with  neuromuscular       \\ndisabilities  to  communicate  with  the  world.  These  brain         \\nsignals  are  translated  as  commands  to  control  the  external          \\ndevices  via  a  computer.  The  BCI  design  involves  the          \\nfollowing  parts:  the  user,  Electroencephalogram(EEG)      \\nacquisition,  signal  processing(preprocessing,  feature     \\nextraction,  classification)  and  the  controlled  device.       \\nElectroencephalogram(EEG)  signals  have  the  capability  of       \\ncapturing  the  brain  information  in  a  fast  dynamic  way  and           \\nhas  a  high  temporal  resolution  but  it  has  a  low  spatial            \\nresolution  along  with  a  high  noise  level.  EEG  recording          \\ninvolves  collecting  data  over  the  scalp  through  electrodes  in          \\nan  extended  10-20  system [1] .  The  EEG  signals  are          \\ndistorted  by  the  electrical  activity  of  levator  muscles(that         \\ncause  eye  blink)  called  artifacts.  Artifacts  can  be  avoided  or           minimized  by  guiding  the  subject  to  be  still,  to  avoid           \\nunnecessary  eye  blink  and  also  proper  grounding  of  EEG          \\nrecorder.  But  avoidance  is  not  the  perfect  solution  for  the           \\nremoval  of  artifacts  as  the  eye  blink  or  other  body           \\nmovements  can’t  be  avoided  for  a  long  time  by  the  subject,            \\nthus,  EEG  signal  processing  is  done  to  remove  these          \\nartifacts.  \\nElectrooculogram(EOG),  is  one  of  the  most  important        \\nocular  artifacts.  EEG  signals  get  highly  contaminated  by         \\nthem.  Eyeblink  occurs  when  a  signal  amplitude  crosses  a          \\ncertain  threshold  and  they  are  suppressed  by  removing  the          \\nsegment  containing  ocular  activity.  Then  all  the  EEG  epochs          \\nwith  signals  higher  than  the  threshold  are  rejected,  but  this           \\ncauses  a  lot  of  portion  to  be  lost.  To  overcome  this  a  new              \\nBlind  Source  Separation(BSS)  technique  called  Principal       \\nComponent  Analysis(PCA)  was  introduced,  still,  this  wasn’t        \\nable  to  remove  the  ocular  artifacts  completely,  especially         \\nwhen  the  amplitude  is  comparable  [2].  To  overcome  the          \\nlimitations  in  PCA,  more  effective  methods  for  the  removal          \\nof  artifacts  are  introduced  which  include  Independent        \\nComponent  Analysis(ICA)  [3],  Independent  Component      \\nAnalysis  Recursive  Least  Squares  (ICA-RLS)  [4],       \\nIndependent  Component  Analysis  Discrete  Wave      \\nTransform(ICA-DWT)  [5],  and  Independent  Component      \\nAnalysis  Least  Mean  Square  (ICA  -  LMS)  [6].  This  work           \\nwill  describe  the  detection  and  removal  of  artifacts  using  the           \\nabove-mentioned  methods  as  feature  extraction  and       \\nfollowed  by  classification  using  SVM  (Support  Vector        \\nMachine)   and   Particle   Swarm   Optimization(PSO).  \\n                                  II.    RELATED   WORKS  \\nMany  methods  are  proposed  for  the  removal  of  artifacts          \\nfrom  EEG  signals.  Li’s  work  on  the  ICA  method  says  that            \\nthis  method  gives  robust  results  and  solves  the  problems  of           \\ndenoising [3] .  This  method  involves  the  decomposition  of         \\nthe  signal  into  independent  components  (ICs).  ICA  method         \\nis  convenient  to  use,  requires  less  memory  space  and  high           \\nconvergence   speed   [3].  \\n ICA-RLS  uses  adaptive  filters.  The  ICA-RLS  method  is          \\nICA  followed  by  RLS  which  involves  the  use  of  adaptive           \\nfilters.  Banghua  Yang’s  work  in  this  method  shows  that  the           \\nocular  artifacts  are  removed  from  the  corrupted  EEG  signals          \\nby  subtracting  the  ocular  activities  from  the  given  corrupted          \\nEEG  signal  where  the  ocular  activity  is  the  sum  of  the            \\nproduct  of  reference  signals  served  by  ICs  and  finite          \\nXXX-X-XXXX-XXXX-X/XX/$XX.00   ©20XX   IEEE  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0aef37ea-12c2-404f-b54e-9c96107e3798', embedding=None, metadata={'page_label': '2', 'file_name': 'IEEE-Conference-Paper-SPIN-Feb-2020.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/IEEE-Conference-Paper-SPIN-Feb-2020.pdf', 'file_type': 'application/pdf', 'file_size': 1997126, 'creation_date': '2024-06-23', 'last_modified_date': '2022-09-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/ \\nimpulse  response  filter’s  kth  coefficient [4] .  The  Discrete         \\nwavelet  transform(DWT)  method  for  the  removal  of        \\n \\ntransformation  with  discrete  data.  But  DWT  cannot        \\ncompletely  remove  the  artifacts  this  is  the  reason  ICA          \\nmethod  is  performed  prior  to  applying  DWT [5] .  ICA-LMS          \\nwas  proposed  by  Saeid  et.  al [6]  is  an  adaptive  algorithm            \\nthat  has  been  recently  used  for  ocular  artifacts  detection  and           \\nremoval.  Md.  kafuil  Islam  in  his  work [7]  with  empirical           \\nmode  decomposition  stated  that  EMD  is  ideal  for  EEG          \\nsignals  as  it  is  developed  to  work  on  non-linear,          \\nnon-stationary  and  stochastic  processes.  EMD  involves       \\ndecomposition  of  the  EEG  signal  into  band-limited        \\ncomponents.  \\n Recently,  Machine  Learning  techniques  have  shown        \\nappreciable  results  for  binary  class  EEG  classification [ 8].         \\nThese  approaches  do  not,  however,  meet  the  needs  of          \\nreal-time,  brain-controlled,  and  neuro-gaming  applications,      \\nwhich  require  multiclass  classification.  Nicolas  et  al. [9]         \\nproposed  a  generalized  method  for  the  classification  of         \\nMultiClass  Motor  Imagery(MI)-based  EEG  signals.  They       \\nhave  used  MIBIF(Mutual-Information  Best  Individual      \\nFeatures)  which  in  turn  extracted  optimal  CSP  features.         \\nYoung  et  al [10]  proposed  a  new  approach  in  which  they            \\nused  the  Filter  Bank  Common  Spatial  Approach  (FBSCP)         \\nand  CSP  for  the  extraction  of  functions  to  preprocess  the           \\nraw  data  accessible  through  a  filter  bandpass.  SVM  (Support          \\nVector  Machine)  which  is  a  well-known  classifier  in         \\nMachine  Learning  was  also  used  to  train  system  based  on           \\nselected   features   extracted   from   EEG   signals.  \\n J.Pylkkonen  introduced  LDA(Linear  Discriminant      \\nAnalysis)  which  was  used  for  classification  tasks  on  the          \\nbasis  of  features  extracted [8] .  In  2016,  Gao  et  al.  worked            \\nwith  Adaboost  (Adaboost  ELM)  classification  capabilities.       \\nThey  extracted  features  using  Kolmogorov  complexity  (Kc).        \\nMeister  et  al.  [9 ]  proposed  an  improvement  in  the          \\npreprocessing  method  by  applying  Joint  Approximate       \\nDiagonalization  (JAD)  which  resulted  in  better  CSP  features         \\nand  hence,  efficient  feature  extraction.  (SBCSP-SBFS)  Novi        \\n[8]  suggested  the  multi-classification  results  of  the  motor         \\nimaging  brain  signals  enhance  specific  subband  spatial        \\npatterns  with  the  sequential  rear-floating  parts.  Later        \\nresearchers [11]  also  applied  KNN,  and  NMPW  classifiers         \\nbut  their  accuracy  was  much  less  than  that  of  LDA  and            \\nSVM.  Yuliang  Ma  et  al. [12]  has  proposed  Support  Vector           \\nMachine   with   Particle   Swarm   Optimization.   \\nSince  the  dataset  is  of  motor  imagery  EEG  signals  many           \\nresearchers  have  applied  different  techniques  for       \\nclassification  of  data.  We,  on  the  other  hand,  removed  ocular           \\nartifacts  and  then  classify  the  EEG  signal,  so  on  doing  this            \\nwe  have  seen  different  classification  accuracy  on  the  data          \\nobtained   after   artifacts   removal   from   all   the   five   methods .  \\n                   III.    MATERIALS   AND   METHODS  \\nA.   Dataset   Description  \\n The  dataset  used  is  BCI  competition  III  dataset  IV  A [13] .             \\nThe  dataset  consisted  of  a  recording  of  the  human  EEG  of  5             \\nhealthy  subjects  collected  using  118  channels  which  had  a          frequency  between  0.05  Hz  to  200Hz.  Each  subject  was          \\nsubjected  to  280  trails.  Event-Related  Potentials(ERP)  are        \\nelectro-cortical  potentials  which  are  measured  in  the  EEGs         \\nbefore  and  after  cued  motor  imagery  events.  The  EEG  had  an            \\nERP  recording  of  the  5  subjects.  For  cued  motor  imagery  the            \\nsubjects  were  asked  to  move  their  right  hand  and  right  foot,            \\ni.e.;  we  had  2  classes.  Out  of  the  5  subjects,  responses  from  2              \\nsubjects  gave  good  training  data  (  subject  1:  80  %;  subject  2:             \\n60%).  The  responses  from  the  remaining  3  subjects  gave  less           \\ntraining  data  (  subject  3:  30%;  subject  4:  20  %;  subject  5:10%             \\n).  Using  a  sampling  rate  of  1000Hz  the  challenge  is  to  detect             \\nand  remove  ocular  artifacts  followed  by  classification  using         \\nthe   little   training   data.  \\nB.   Background   Knowledge  \\n The  application  of  ICA  on  the  datasets  is  done  with  the             \\nhelp  of  a  very  effective  toolbox  EEGLAB  v.  2019.0  on           \\nMATLAB  R2019a.  The  dataset  is  imported  individually        \\nalong  with  event  makers  and  channel  locations  to  the          \\nEEGLAB.  This  dataset  is  re-referenced  or  downsampled        \\nusing  average  reference  property.  Kurtosis  measure  of        \\nidentification  is  used  for  bad  channel  analysis,  and  removal          \\nusing  the  channel  plot  with  a  z  threshold  of  5.  Each  IC             \\nconsists  of  data  from  118  channels  that  are  present  in  the            \\nDataset  IV  A  of  BCI  Competition  III.  The  final  step  in  the             \\nICA  is  the  identification  of  bad  components  using  the          \\ncomponent  activations  and  component  spectra  maps  that        \\nconsists  of  the  power  spectral  density  (DB)  v/s  frequency          \\n(Hz)  and  ERP  (Event-Related  Potential)  Images.  The        \\nidentification  of  bad  ICs  using  these  is  completely         \\nsubjective.  Once  the  bad  components  are  identified,  they  are          \\nrecorded  and  without  removing  these  bad  components,  the         \\ndata  is  further  processed  using  one  of  the  feature  detection           \\nmethods .   \\nC.   Feature   Detection   Techniques  \\n1)   Independent   Component   Analysis(ICA):   \\n ICA  is  an  effective  BSS  identification  and  removal          \\ntechnique  [3].  In  ICA  the  multichannel  EEG  data  are          \\nlinearly  broken  down  to  generate  a  maximum  of  individual          \\nelements,  temporarily  independent  and  spatially  defined.       \\nThe  ICs  subject  to  artifacts  are  set  to  zero  and  the  majority             \\nof  the  ICs  are  projected  back  onto  the  scalp  to  collect  the             \\ntrue  EEG  information.  These  IC  components  are  subjected         \\nto  artifacts.  Here,  one  component  cannot  infer  with  the  data           \\nof  another  component.  So  statistically  the  joint  probability  is          \\ngiven  as  the  product  of  the  probability  of  each  of  the            \\nindependent  components.  If  there  are  n  independent  source         \\nsignals   ,           i    =1,2,3,.....n   and   1 x(t)i . ≤t ≤T \\n \\n         p(x(t)) (x(t)) = ∏n\\ni=1pi                                     (1)  \\n                  y(t) x(t)  =A                                       (2)  \\n Where  is  an  scalar  matrix  where  d  is   A    х dn       \\ndimensional   data   vector   and   .  d ≥c \\n artifacts is a common denoising tool and  is a continuous', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5b3b8f73-a235-41c8-a52b-4472743781d4', embedding=None, metadata={'page_label': '3', 'file_name': 'IEEE-Conference-Paper-SPIN-Feb-2020.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/IEEE-Conference-Paper-SPIN-Feb-2020.pdf', 'file_type': 'application/pdf', 'file_size': 1997126, 'creation_date': '2024-06-23', 'last_modified_date': '2022-09-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/ \\n is  determined  by  maximum  likelihood  techniques.  Let  A        \\nthe  source  of  density  be  estimated  by  the  parameter          (y;) p︿a\\nwhere reduces  the  difference  between  the  estimate  and   a        \\nthe   source   distribution.   \\n Thus is  an  estimate  of  and a is  basis  vector  of   (y;) p︿a   (y)p       \\nA.  \\n2)  Independent  Component  Analysis-Recursive  Least      \\nSquare(ICA-RLS):   \\n This  algorithm  is  a  combination  of  both  ICA  and  RLS            \\n[4].  RLS  technique  uses  adaptive  filters,  where  the  adaptive          \\nfilter   is   given   by   \\n               (3) (n) (k) r(n )                              vj= ∑h\\nk=1hj j −k+1  \\n Where  represents  the  ocular  activities  of  EEG   (n)vj       \\nsignal  y(n).  Each  EEG  signal  also  contains  nuclear      (n)yl    \\nactivity  along  with  ocular  activity .  (n)xj     (n)vj \\nare  the  reference  signals  served  by (n),(n).....r(n) r1r2 m       \\nocular   ICs   for   the   adaptive   filter.  \\n The  EEG  signals  are  hindered  by  the  artifacts  and  it  is             \\ngiven  by  e(n),  e(n)  is  corrected  by  subtracting  ocular          \\nactivities     from   EEG   signal   . (n)vj(n)yl \\n                                         (4)               e(n) (n) (n)  =yl − ∑m\\nj=1vj  \\n RLS  technique  is  used  for  adjusting  for  canceling        (n)hj   \\nas  many  ocular  activities  as  possible  and  this  is  done  by            \\nreducing  the  sum  e(n)  of  the  weighted  square  errors  as  less            \\nas   possible   and   is   given   by-  \\n                      (5) (n)  εn=e2+λ (n )...... e(L) e2−1+. +λnL −2 \\n The  ‘forgetting  factor’  0  <  <  1  is  responsible  for       λ      \\ngradually   reducing   the   effects   of   previous   errors.  \\nSo,  the  ICA-RLS  technique  is  the  first  ICA  followed  by           \\nRLS.  \\n \\nFig1.   Block   diagram   of   ICA-RLS.  \\n3)Independent  Component  Analysis-  Discrete  Wavelet      \\nTransform:   Wavelet   transform(WT):  \\n It  can  be  defined  as  the  product  of  wavelet  function’s  Ψ             \\n(t)  shifted  and  time-scaled  version  and  the  signal  f(t).  The           \\nWT  splits  the  signal  into  a  series  of  coefficients  on  various            scales  which  then  reveal  the  wavelet-like  signal.  A  standard          \\nDWT  is  a  grid  of  dyadic  amplitude  and  time  scales  of  2j  and              \\ntime-shift   b   =   k2j,   with   both   j   and   k   integers.  \\nThe   wavelet   function   is   given   as-  \\n                                                        (6) (t) (2t) ψj,k=2j2/ j2/−k  \\n Where  j  indicates  the  degree  of  resolution  and  k  is  the             \\ntime  translation.  This  is  named  dyadic  scaling  because  it          \\nrequires   the   scaling   factor   to   be   2.  \\n The  following  formula  will  express  the  wavelet         \\ndecomposition:  \\n             (7) (t) [c ϕ(t)] ψ(2jt)      f= ∑+ ∞\\nk= ∞ −k −k+ ∑+ ∞\\nk= ∞ −∑+ ∞\\nj=0dj,k −k  \\n In  which (t)  is  the  scaling  function,  and  in  which     ϕ         ck \\nand ,  k  are  respectively  the  coarse  and  accurate  expansion  dk         \\ncoefficients.  \\n     The   input   and   output   equations   for   DWT   is   given   as-  \\n                          g[k]                   (8) [n] [2n ] xa,L= ∑n\\nk=1xa1,L − −k  \\n                        h[k]                     (9) [n] [2n ] xa,H= ∑n\\nk=1xa1,L − −k  \\n Where  g[n]  is  just  like  scaling  function  which  is  a            \\nlow-pass  filter  while  h[n]  is  just  like  wavelet  function  which           \\nis  a  high-pass  filter.  Generally,  DWT  is  accompanied  by          \\nthreshold  selecting  criteria.  ICA  algorithm  is  also  required         \\nwith  DWT  because  DWT  is  not  capable  of  completely          \\nextracting  artifacts  interfering  with  an  EMG  signal  from  a          \\nspectral   domain   like   ECG   [5].  \\n4)Independent  Component  Analysis  -  Least  Mean       \\nSquare(ICA-LMS):   \\n Least  Mean  Square  Method  uses  an  adaptive  filter          \\nwhich  adjusts  the  filter  coefficients  in  accordance  with  an          \\nadaptive  algorithm  [6].  This  method  simplifies  calculations        \\nand  speeds  up  the  process  by  estimating  gradient  with  the           \\nhelp  of  instantaneous  values  of  the  correlation  matrix  of  tap           \\ninputs  and  also  by  calculating  the  cross-correlation  vector         \\nbetween  tap  weights  and  the  desired  response.  ICA-LMS  is          \\nconsidered  an  efficient  method  to  remove  ocular  artifacts         \\nwhile  working  on  Motor-Imagery  Data.  For  Motor  Imagery         \\nData  (especially  hand  and  foot  movement),  suffers  from         \\nocular   artifacts   much   more   than   any   other   EEG   signal.  \\n5)Empirical  Mode  Decomposition  -  Independent      \\nComponent   Analysis   (EMD-ICA):   \\n The  EMD  is  a  method  that  has  been  developed  to  operate             \\non  non-stationary,  non-linear,  stochastic  processes,  making       \\nit  ideal  for  EEG  signals.EMD  doesn’t  have  many  online          \\napplications   as   it   as   high   computational   complexity.  \\n The  EMD  algorithm  breaks  down  nonlinear  data  signals          \\nand  non-stationary  data  into  a  band-limited  component        \\nsummary,  c[n]  called  IMF  functions  [7].  The  zero  mean          \\namplitude   components   are   determined   by   these   IMFs.  \\n \\n            s(t)   =   (t),   i   =   1                                  (10) ∑N\\ni=1ci \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2538af1e-dd47-409b-aa46-093b04089a27', embedding=None, metadata={'page_label': '4', 'file_name': 'IEEE-Conference-Paper-SPIN-Feb-2020.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/IEEE-Conference-Paper-SPIN-Feb-2020.pdf', 'file_type': 'application/pdf', 'file_size': 1997126, 'creation_date': '2024-06-23', 'last_modified_date': '2022-09-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/ \\n                           s(t)   =   (t)   +   (t)                               (11) ∑N\\ni=1ciri  \\nN   -   IMFs   extracted   count,   (t)   -   residue ri \\n The  main  function  of  EMD  is  to  deal  with  the  extraction             \\nof  IMFs  which  involves  the  removal  of  higher-level         \\ncomponents.  The  calculation  of  envelopes  is  done  through         \\ninterpolation.   \\n In  order  to  detect  the  abnormal  elements  in  the  signal            \\nthat  can  be  recognized  as  IMFs  and  can  also  be  rejected  to             \\nclean  the  signal,  decomposition  achieved  via  EMD  is  used.          \\nThe  EEG  signal  is  then  subjected  to  ICA  when  the  IMFs  are             \\nbroken  down  into  separate  components  (ICs).  Using  ICA,         \\nweak  components  are  detected  and  cleaned  by  removing  ICs          \\nand   obtaining   a   clean   signal.  \\n \\n \\nFig2.   The   variation   in   ERP   image   of   channel   1   after   the   EMD   -   ICA  \\nimplementation  \\n The  EMD  method  decomposes  the  data  into  IMFs  which           \\nare  further  identified  by  decomposition  into  ICs  as  bad          \\nIMFs   based   on   a   certain   criterion   which   is   subjective.  \\nD.    Classification   Method  \\n As  mentioned,  BCI  Competition  III  Dataset  IV  A          \\nconsists  of  EEG  signals  recorded  from  5  subjects  when  they           \\nwere  told  to  move  right  hand  and  foot.  Hence,  this  will  be  a              \\ncase   of   binary   classification.   \\n SVM-PSO  method  will  overcome  the  problem  of         \\nchoosing  a  kernel  function  for  SVM  and  the  mandatory          \\nprior  knowledge  of  the  distribution  characteristic  of  the         \\nsignal.  This  algorithm  selects  the  best  kernel  function  with          \\npenalty  parameters  and  hence  the  accuracy  is  improved  as          \\ncompared   to   the   traditional   SVMs.  \\nCommon   Spatial   Pattern   (CSP)   -   \\n The  principle  component  forms  the  base  of  the  CSP           \\nmethod.  Previously,  CSP  has  been  used  in  tasks  associated          \\nwith  object  recognition  and  face  recognition,  and  anomaly         \\ndetection.  Recently,  it  was  successfully  applied  to  problems         \\nrelated  to  brain-computer  interfaces.  In  order  to  extract         \\nfeatures  using  CSP,  firstly  the  EEG  signals  are  passed          \\nthrough  CSP  filter,  to  make  the  data-set  such  that  it  will  be             \\ndivided  into  two  classes,  one  with  maximum  variance  and          \\nsecond   with   the   minimum   variance.  \\n Before  decomposition  of  features  using  Principal        \\nComponent  Analysis,  the  covariances  for  both  the  classes         \\nwere  measured.  Also,  the  eigenvectors  of  the  covariance         \\nmatrix  are  the  same.  Hence,  if  one  feature  comes  out  to  be             \\nmaximal  other  will  be  automatically  classified  to  minimal.         \\nThat’s  why  both  the  classes  are  easily  separable  when  we           \\napply   CSP.  \\nSVM(Support   Vector   Machine):-    SVM  is  a  discriminative  classifier  and  is  normally          \\ndefined  by  a  hyperplane,  which  is  a  line  dividing  a  plane            \\ninto  two  parts,  and  this  two-part  will  represent  two  classes.           \\nAs  mentioned  by  Chandaka [14] ,  non-linear  classification        \\nproblems  can  be  converted  to  linear  classification  problems         \\njust  by  raising  a  degree.  Also  in  case  of  non-linear           \\nclassification  problems  in  SVM,  instead  of  inner  product         \\ncomputation,   kernel   function   is   used.  \\n \\nPSO   (Particle   Swarm   Optimization):-   \\n PSO  is  extensively  used  nowadays  due  to  the          \\nindependence  of  the  target  optimization  of  PSU  and         \\nParallelism.  \\n1.Initialization:-  All  the  input  vectors  must  be        \\ninitialized  with  setting  the  parameters  and  initial        \\nweight  of  parameters.  At  the  end  of  this  step,  the           \\nlargest  number  of  iterations  will  be  determined  and         \\nthe   size   of   the   population   will   also   be   reported.  \\n2.Calculate  Fitness:-  After  training  the  model  with        \\nthe  help  of  training  data-set,  calculate  the  overall         \\nfitness   using   fitness   function.  \\n;   j   =   1,2,3,....2m fj =  æ ç ç èvarY(j)\\nog (var(Y)) ∑2m\\nk = 1lk  ö ÷ ÷ ø     (12)  \\nwhere   Y   is   class,   and   j   is   class   number.  \\n3.Adjusting:-  Global  and  personal  best  positions  are        \\nadjusted  in  accordance  with  the  fitness  value  of         \\neach   particle.  \\n4.Updating:-   Velocity   and   Position   are   updated.  \\n5.Determination:-  Stop  after  a  maximum  number  of        \\niterations   are   over   or   error   condition   is   fulfilled.  \\n6.Classification:-  This  is  the  final  step  where  we         \\nhave  obtained  the  most  optimal  kernel  parameter        \\nand  penalty  factor,  hence  the  SVM  classifier  will         \\nretain  training  samples.  Then  we  will  use  class         \\nFig3.   Flowchart   of   PSO   optimized   SVM   parameters  \\n                                  IV.    RESULTS  \\n The  use  of  hybrid  methods  gives  better  accuracy  and           \\noutput  than  the  normal  and  standardized  method  of  ICA.The          \\ncomparison  of  spectral  component  activations  of  the  first  35          \\n prediction for containing best classifiers.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b880a18c-ccba-4e43-8a66-2fc83d82b467', embedding=None, metadata={'page_label': '5', 'file_name': 'IEEE-Conference-Paper-SPIN-Feb-2020.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/IEEE-Conference-Paper-SPIN-Feb-2020.pdf', 'file_type': 'application/pdf', 'file_size': 1997126, 'creation_date': '2024-06-23', 'last_modified_date': '2022-09-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/ \\ncomponents  for  the  ICA  and  the  first  35  components  after           \\napplication  of  various  methods  along  with  ICA  result  in  the           \\nobservation  of  a  significant  improvement  over  the  standard         \\nICA  method.  The  output  from  preprocessing  is  supplied  as          \\nan   input   to   the   final   classification   through   the   SVM-PSO.  \\n After  performing  the  artifact  removal  task,  binary         \\nclassification  using  SVM-PSO  was  performed  to  classify        \\nthe  given  two  classes.  We  have  applied  classification  on  the           \\noutput  obtained  after  artifacts  removal  techniques.  As,  ICA-         \\nRLS  gives  the  highest  classification  accuracy  and  hence  we          \\ncan  conclude  that  it  must  have  removed  a  large  number  of            \\nocular  artifacts  and  is  the  best  method  to  remove  ocular           \\nartifacts  for  the  given  dataset.  The  detailed  analysis  of  the           \\nclassification   is   mentioned   in   Table   1.  \\n  \\n  TABLE-I  \\nCLASSIFICATION   ACCURACY   OF   PROPOSED   METHODS  \\n \\nMethods  used  to    \\nRemove   Artifacts  Classification  \\nAccuracy  \\nICA-RLS  92.94%  \\nICA-DWT  91.23%  \\nEMD-ICA  90.67%  \\nICA-LMS  90.46%  \\nICA  87.04%  \\n \\nTABLE-II  \\nCLASSIFICATION   ACCURACY   OF   ALREADY   EXISTING   METHODS  \\n \\nAuthors  Year   Feature  \\nExtraction  Classificati \\non   Method  Accurac \\ny  \\nProposed  2019  ICA-RLS  +   \\nCSP  SVM-PSO  92.94%  \\nKervin  &   \\nSubasi  \\n[15]  2018  Empirical  \\nMode  \\nDecompositio \\nn,   DWT  k-NN  91.5%  \\nZhou  et   \\nal.   [16]  2017  DWT  &   \\nHilbert  \\nTransform  RNN  \\nLSTM  \\nClassifier  91.43%  \\nBaig  et  al    \\n[17].  2017  CSP  SVM-PSO  90.4%  \\nYu  et  al.    \\n[10]  2014  CSP  SVM  76.34%  \\nBermudez  \\n&Garcia  \\n[18]  2012  AAR  \\nModelling,  \\nPSD  LDA  69.4%  \\n  \\nFig4.   Representation   of   channels   before   running   ICA  \\n Figure  4  shows  the  channel  activations  and  the  channels           \\nthat  remain  after  filtering  through  the  kurtosis  measure.         \\nThese  channels  are  now  made  to  be  passed  through  ICA  in            \\norder   to   obtain   independent   components.  \\nFig5.   Representation   of   components   using   Independent   Component  \\nAnalysis   (ICA)  \\n Figure  5  shows  the  first  35  independent  components          \\nafter  the  decomposition  through  ICA.  These  ICs  contain         \\ndata  from  all  of  the  channels  of  the  initial  data.  These  ICs             \\nare  used  to  obtain  blind  source-separated  components  from         \\nthe   initial   data.  \\n \\n                      Fig5.1.   Output   and   Input   comparison   of   ICA  \\n Figure  5.1  shows  the  comparison  of  different  methods          \\nwith  the  component  activations  obtained  after  applying  ICA.         \\nThese  comparatives  are  a  result  of  the  selection  of          \\ncomponents  that  is  done  using  various  methods.  As  a  result           \\ncomponents  are  decomposed  differently  from  each  method.        \\nThe  components  with  the  most  noise  levels  are  selected  and           \\nmarked  for  rejection.  The  graph  that  shows  red  curves  are           \\nobtained   after   the   removal   of   these   components.  \\n \\n           Fig5.2.   Output   and   Input   comparison   of   ICA-LMS  \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c4bc34a5-fba7-47e7-b444-0e9753697e87', embedding=None, metadata={'page_label': '6', 'file_name': 'IEEE-Conference-Paper-SPIN-Feb-2020.pdf', 'file_path': '/Users/mohitsarin/Documents/GitHub/RAG-LLM-Application/data/IEEE-Conference-Paper-SPIN-Feb-2020.pdf', 'file_type': 'application/pdf', 'file_size': 1997126, 'creation_date': '2024-06-23', 'last_modified_date': '2022-09-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/ \\nFig5.3.   Output   and   Input   comparison   of   ICA-RLS  \\n \\nFig5.4.   Output   and   Input   comparison   of   ICA-DWT  \\n \\nFig5..5.   Output   and   Input   comparison   of   EMD-ICA  \\n \\nFig6.   Variation   of   log   PSD   v/s   Frequency   curve   along   with   components.   \\n In  Figure  6  Components  with  frequency  6  Hz,  10  Hz,            \\n22  Hz  are  kept  as  reference  components  and  based  on  these            \\nreference  components  the  log  PSD  v/s  frequency  curve  is          \\nfurther  evaluated  to  look  for  the  components  that  are  ‘bad’           \\nor   ‘erroneous’   and   should   be   rejected.  \\n                              V.    CONCLUSIONS  \\n This  work  is  a  conclusive  study  of  5  different  methods            \\nthat  have  been  used  to  analyze  the  EEG  data.  These  methods            \\nhave  shown  a  significant  improvement  over  the  standard         \\nICA  procedures.  From  the  classification  results,  we  can         \\nconclude  that  ocular  artifacts  have  been  removed  by  hybrid          \\nmethods  with  greater  accuracy  than  the  standard  ICA         \\nmethod.  Thus,  hybrid  methods  in  this  work  perform  better          \\nthan  standard  ICA.  For  classification,  we  have  used         \\nSVM-PSO  and  we  have  performed  feature  detection  using         \\nCSP   and   various   hybrid   methods.  \\n This  work  can  be  extended  by  performing  classification          \\nof  multi-class  motor  imagery  EEG  signals.  In  this  work,  we           have  removed  ocular  artifacts  only,  since  motor  imagery         \\nsignals  are  produced  by  muscular  signals,  hence  muscular         \\nartifacts  are  very  difficult  to  remove  as  we  have  to           \\ndistinguish  between  voluntary  and  involuntary  muscular       \\nmovements.  We  would  extend  this  research  on  multi-class         \\ndata  and  also  incorporate  muscle  artifacts  removal        \\ntechniques.   \\n                                          R EFERENCES  \\n[1] R.  Patel,  M.  P.  Janawadkar,  S.  Sengottuvel,  K.  Gireesan,  and  T.  S.             \\nRadhakrishnan,  “Suppression  of  Eye-Blink  Associated  Artifact  Using        \\nSingle-Channel  EEG  Data  by  Combining  Cross-Correlation  With        \\nEmpirical  Mode  Decomposition,” IEEE  Sensors  Journal ,  vol.  16,  no.          \\n18.   pp.   6947–6954,   2016   [Online].   \\n[2] Z.  Tiganj,  M.  Mboup,  C.  Pouzat,  and  L.  Belkoura,  “An  Algebraic            \\nMethod  for  Eye  Blink  Artifacts  Detection  in  Single-Channel  EEG          \\nRecordings,”    IFMBE   Proceedings .   pp.   175–178,   2010   [Online].   \\n[3] M.  K.  Islam,  A.  Rastegarnia,  and  Z.  Yang,  “Methods  for  artifact            \\ndetection  and  removal  from  scalp  EEG:  A  review,” Neurophysiologie          \\nClinique/Clinical   Neurophysiology ,   vol.   46,    2012.  \\n[4] B.  Yang  and  L.  He,  “Removal  of  ocular  artifacts  from  EEG  signals             \\nusing  ICA-RLS  in  BCI,” 2014  IEEE  Workshop  on  Electronics,          \\nComputer,   and   Applications .   2014   [Online].   \\n[5] M.  Li,  Y.  Cui,  and  J.  Yang,  “Automatic  Removal  of  Ocular  Artifact             \\nfrom  EEG  with  DWT  and  ICA  Method,” Applied  Mathematics  &           \\nInformation   Sciences ,   vol.   7,   no.   2.   pp.   809–816,   2013   [Online].   \\n[6] S.  Mehrkanoon,  M.  Moghavvemi,  and  H.  Fariborzi,  “Real-time  ocular          \\nand  facial  muscle  artifacts  removal  from  EEG  signals  using  LMS           \\nadaptive  algorithm,” 2007  International  Conference  on  Intelligent  and         \\nAdvanced   Systems .   2007   [Online].   \\n[7] Mijović,  Bogdan,  et  al.  \"Combining  EMD  with  ICA  for  extracting           \\nindependent  sources  from  single  channel  and  two-channel  data.\" 2010          \\nAnnual  International  Conference  of  the  IEEE  Engineering  in  Medicine          \\nand   Biology .   IEEE,   2010.  \\n[8] Q.  Novi,  C.  Guan,  T.  H.  Dat,  and  P.  Xue,  “Sub-band  Common  Spatial              \\nPattern  (SBCSP)  for  Brain-Computer  Interface,” 2007  3rd        \\nInternational   IEEE/EMBS   Conference   on   Neural   Engineering .   2007 .  \\n[9] K.  K.  Ang,  Z.  Y.  Chin,  C.  Wang,  C.  Guan,  and  H.  Zhang,  “Filter  Bank                \\nCommon  Spatial  Pattern  Algorithm  on  BCI  Competition  IV  Datasets          \\n2a   and   2b,”    Frontiers   in   Neuroscience ,   vol.   6.   2012   [Online].   \\n[10] Yong,  X.  and  Menon,  C.,  2015.  \"EEG  classification  of  different           \\nimaginary   movements   within   the   same   limb\".    PloS   one ,    10 (4).  \\n[11] A.  Yazdani,  T.  Ebrahimi,  and  U.  Hoffmann,  “Classification  of  EEG           \\nsignals  using  Dempster  Shafer  theory  and  a  k-nearest  neighbor          \\nclassifier,” 2009  4th  International  IEEE/EMBS  Conference  on  Neural         \\nEngineering .   2009   [Online].  \\n[12] Y.  Ma,  X.  Ding,  Q.  She,  Z.  Luo,  T.  Potter,  and  Y.  Zhang,              \\n“Classification  of  Motor  Imagery  EEG  Signals  with  Support  Vector          \\nMachines  and  Particle  Swarm  Optimization,” Computational  and        \\nMathematical  Methods  in  Medicine ,  vol.  2016.  pp.  1–8,  2016          \\n[Online].   \\n[13] Available:    http://www.bbci.de/competition/iii/#data_set_iva .   \\n[14] S.  Chandaka,  A.  Chatterjee,  and  S.  Munshi,  “Cross-correlation  aided          \\nsupport  vector  machine  classifier  for  classification  of  EEG  signals,”          \\nExpert   Systems   with   Applications ,   vol.   36 .  \\n[15] Hazarika,  A.,  &  Bhuyan,  M.  (2019).  A  Feature  Fusion-Based           \\nDiscriminant  Learning  Model  for  Diagnosis  of  Neuromuscular        \\nDisorders  Using  Single-Channel  Needle  Electromyogram  Signals.  In        \\nIntelligent   Data   Analysis   for   Biomedical   Applications .  \\n[16]  Zhou,  Jie,  et  al.  \"Classification  of  motor  imagery  EEG  using  wavelet             \\nenvelope  analysis  and  LSTM  networks.\" 2018  Chinese  Control  And          \\nDecision   Conference   (CCDC) .   \\n[17]  Baig,  Muhammad  Zeeshan,  et  al.  \"Differential  evolution  algorithm  as           \\na  tool  for  optimal  feature  subset  selection  in  motor  imagery  EEG.\"            \\nExpert   Systems   with   Applications    90   (2017):   184-195.  \\n[18]  Rodríguez-Bermúdez,  Germán,  and  P.J.  García-Laencina.  \"Automatic        \\nand  adaptive  classification  of  electroencephalographic  signals  for        \\nbrain   computer   interfaces.”    Journal   of   medical   system   2012.  \\n \\n ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking whether documents are created properly or not\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************caD2. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m index\u001b[38;5;241m=\u001b[39m\u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/llama_index/core/indices/base.py:145\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, service_context, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[1;32m    138\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[1;32m    139\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     transformations,\n\u001b[1;32m    141\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    143\u001b[0m )\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:75\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     69\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m embed_model_from_settings_or_context(Settings, service_context)\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/llama_index/core/indices/base.py:94\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m---> 94\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:308\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    301\u001b[0m     node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mEMBED) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[1;32m    302\u001b[0m ):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot build index from nodes with no content. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure all nodes have content.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:280\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     run_async_tasks(tasks)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_nodes_to_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:233\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[0;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size):\n\u001b[0;32m--> 233\u001b[0m     nodes_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_with_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     new_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39madd(nodes_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mstores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override:\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:141\u001b[0m, in \u001b[0;36mVectorStoreIndex._get_node_with_embedding\u001b[0;34m(self, nodes, show_progress)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_node_with_embedding\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    132\u001b[0m     nodes: Sequence[BaseNode],\n\u001b[1;32m    133\u001b[0m     show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    134\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[1;32m    135\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get tuples of id, node, and embedding.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    Allows us to store these nodes in a vector store.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    Embeddings are called in batches.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     id_to_embed_map \u001b[38;5;241m=\u001b[39m \u001b[43membed_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/llama_index/core/indices/utils.py:138\u001b[0m, in \u001b[0;36membed_nodes\u001b[0;34m(nodes, embed_model, show_progress)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         id_to_embed_map[node\u001b[38;5;241m.\u001b[39mnode_id] \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39membedding\n\u001b[0;32m--> 138\u001b[0m new_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_embedding_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts_to_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_id, text_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids_to_embed, new_embeddings):\n\u001b[1;32m    143\u001b[0m     id_to_embed_map[new_id] \u001b[38;5;241m=\u001b[39m text_embedding\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/llama_index/core/base/embeddings/base.py:332\u001b[0m, in \u001b[0;36mBaseEmbedding.get_text_embedding_batch\u001b[0;34m(self, texts, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    324\u001b[0m     EmbeddingStartEvent(\n\u001b[1;32m    325\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39mmodel_dict,\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    327\u001b[0m )\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    329\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING,\n\u001b[1;32m    330\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()},\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 332\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m     result_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    334\u001b[0m     event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    335\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    336\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: cur_batch,\n\u001b[1;32m    337\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: embeddings,\n\u001b[1;32m    338\u001b[0m         },\n\u001b[1;32m    339\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/llama_index/embeddings/openai/base.py:429\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_text_embeddings\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get text embeddings.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03mBy default, this is a wrapper around _get_text_embedding.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03mCan be overridden for batch queries.\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    428\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client()\n\u001b[0;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text_engine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/tenacity/__init__.py:332\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\n\u001b[1;32m    329\u001b[0m     f, functools\u001b[38;5;241m.\u001b[39mWRAPPER_ASSIGNMENTS \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__defaults__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__kwdefaults__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    330\u001b[0m )\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/tenacity/__init__.py:469\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/tenacity/__init__.py:370\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    368\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 370\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/tenacity/__init__.py:392\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[0;32m--> 392\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/tenacity/__init__.py:472\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 472\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    474\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/llama_index/embeddings/openai/base.py:180\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(client, list_of_text, engine, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_text) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch size should not be larger than 2048.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m list_of_text \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m list_of_text]\n\u001b[0;32m--> 180\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_of_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [d\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/openai/resources/embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    108\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    109\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/openai/_base_client.py:1250\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1238\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1246\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1247\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1248\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1249\u001b[0m     )\n\u001b[0;32m-> 1250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/openai/_base_client.py:931\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    924\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    929\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    930\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-LLM-Application/venv/lib/python3.10/site-packages/openai/_base_client.py:1030\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1029\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1033\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1034\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1038\u001b[0m )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************caD2. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "index=VectorStoreIndex.from_documents(documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
